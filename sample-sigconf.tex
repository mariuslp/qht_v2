\documentclass[sigconf]{acmart}

\usepackage{xspace}
\usepackage{centernot}
\usepackage{amsmath}

\newcommand{\FPR}{\textsf{FPR}\xspace}
\newcommand{\FNR}{\textsf{FNR}}
\newcommand{\DUPLICATE}{\textsf{DUPLICATE}\xspace}
\newcommand{\UNSEEN}{\textsf{UNSEEN}\xspace}
\newcommand{\todo}[1][]{{\color{red}{TODO: #1}}}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08emT\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
 
%\copyrightyear{2018}
%\acmYear{2018}
%\setcopyright{acmlicensed}
%\acmConference[Woodstock '18]{Woodstock '18: ACM Symposium on Neural Gaze Detection}{June 03--05, 2018}{Woodstock, NY}
%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection, June 03--05, 2018, Woodstock, NY}
%\acmPrice{15.00}
%\acmDOI{10.1145/1122445.1122456}
%\acmISBN{978-1-4503-9999-9/18/06}

%
% These commands are for a JOURNAL article.
%\setcopyright{acmcopyright}
%\acmJournal{TOG}
%\acmYear{2018}\acmVolume{37}\acmNumber{4}\acmArticle{111}\acmMonth{8}
%\acmDOI{10.1145/1122445.1122456}

%
% Submission ID. 
% Use this when submitting an article to a sponsored event. You'll receive a unique submission ID from the organizers
% of the event, and this ID should be used as the parameter to this command.
%\acmSubmissionID{123-A56-BU3}

%
% The majority of ACM publications use numbered citations and references. If you are preparing content for an event
% sponsored by ACM SIGGRAPH, you must use the "author year" style of citations and references. Uncommenting
% the next command will enable that style.
%\citestyle{acmauthoryear}


\begin{document}


\title{Adapting Duplicate Filters to a Sliding Window Context}

%
% The "author" command and its associated commands are used to define the authors and their affiliations.
% Of note is the shared affiliation of the first two authors, and the "authornote" and "authornotemark" commands
% used to denote shared contribution to the research.
\author{R\'emi G\'eraud-Stewart}
\affiliation{%
  \institution{D\'epartement d'informatique de l'ENS, \'Ecole normale sup\'erieure, CNRS, PSL Research University}
  \city{Paris}
  \state{France}
}
\affiliation{%
	\institution{Ingenico Labs Advanced Research}
	\city{Paris}
	\state{France}
}
\email{remi.geraud@ens.fr}

\author{Marius Lombard-Platet}
\affiliation{%
  \institution{D\'epartement d'informatique de l'ENS, \'Ecole normale sup\'erieure, CNRS, PSL Research University}
  \city{Paris}
  \country{France}
}
\affiliation{%
\institution{Be-Studys}
\city{Geneva}
\country{Switzerland}
}
\email{marius.lombard-platet@ens.fr}

\author{David Naccache}
\affiliation{%
  \institution{D\'epartement d'informatique de l'ENS, \'Ecole normale sup\'erieure, CNRS, PSL Research University}
  \city{Paris}
  \country{France}
}
\affiliation{%
	\institution{Ingenico Labs Advanced Research}
	\city{Paris}
	\state{France}
}
\email{david.naccache@ens.fr}

%
% By default, the full list of authors will be used in the page headers. Often, this list is too long, and will overlap
% other information printed in the page headers. This command allows the author to define a more concise list
% of authors' names for this purpose.
%\renewcommand{\shortauthors}{G\'eraud-Stewart et al.}

\begin{abstract}
We introduce a generic framework for adapting duplicate filters for sliding windows. Most papers on the literature focus on the fact that a filter must detect a duplicate on the whole stream. In this paper, we recall from previous work that this problem is ill-defined, and that


\todo 
\end{abstract}

%
% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
% Please copy and paste the code instead of the example below.
%
\begin{CCSXML}

\end{CCSXML}


\keywords{datasets, neural networks, gaze detection, text tagging}


\maketitle

\section{Introduction}
\subsection{Motivation}
As the Internet has grown in popularity, data streams have become bigger and bigger. However, while the datasets grow, the need for a quick answer has remained the same. Yet, in many cases, it is not possible to find the optimal solution by storing all the data, may it be for time limitation or memory limitations. Hence, researchers have gone forward and proposed approximate algorithms, that thrive to get as close as possible to the solution, despite the lack of memory and CPU time. 

Such algorithms are present in network management \cite{10.5555/647912.740658}, in credit card fraud \cite{DBLP:journals/corr/abs-1709-08920}, phone calls data mining \cite{10.1145/347090.347094} and many other topics. A discussion about algorithms on large data streams can be found in \cite{10.1145/776985.776986}. 

In this paper, we focus on the topic of duplicate detection. While many authors in the literature tackle the problem of approximate duplicate detection with fuzzy matching, \cite{10.5555/1287369.1287420,1410199,10.1109/ICDE.2011.5767865,10.1109/ICDE.2012.20,Monge97anefficient}, we instead focus on \emph{exact} duplicate detection, in which an element is a duplicate if and only if it has already appeared in the stream. One can prove (see for instance \cite[][Theorem 2.1]{10.1145/3297280.3297335}) that perfect detection of duplicates over an alphabet of $U$ letters requires $U$ bits of memory. Yet, in the case of extremely large alphabets, it is not possible or desirable to use that amount of memory. 

As it turns out, (exact) duplicate detection has many real-life use cases, and can sometimes play a critical role, for instance in cryptographic schemes where all security and secrecy fall apart as soon as a random nonce is used twice, such as the Elgamal scheme \cite{10.1007/3-540-39568-7_2}. Other uses include improvements over caches \cite{4484874}, duplicate clicks \cite{10.1145/1060745.1060753}, and others.

\todo[continuer].

\subsection{Contributions}

\subsection{Related Work}

\subsection{Organisation}


\section{Duplicate Detection and Filter Saturation}
\subsection{Notations and Initial Problem Statement}
Let us consider an alphabet $\Gamma$ of $U$ elements. A stream $E$ of $n$ elements of $\Gamma$ is noted $E = \{e_1, \dotsc, e_n\}$. If the stream is infinite or of unknown size, we note $E = \{e_1, \dotsc, e_n, \dotsc\}$.

\begin{definition}[Duplicate element, unseen element]
Let $E = \{e_1, \dotsc, e_n, \dotsc\}$ be a stream over $\Gamma$. $e_j$ is called a \emph{duplicate} in $E$ if and only if $\exists i < j, e_i = e_j$. Otherwise, $e_j$ is said to be \emph{unseen}.
\end{definition}

A filter $\mathcal F$ is defined by $\mathcal F = (\mathcal S, \mathsf{Insert}, \mathsf{Detect})$. Furthermore,

\begin{itemize}
	\item $\mathcal S \in \mathcal M$ represents the current state of the filter amongst all possible memory states $\mathcal M$.
	\item $\mathsf{Detect}: \mathcal M \times \Gamma \rightarrow \{\DUPLICATE, \UNSEEN\}$ is an algorithm that given the current state $\mathcal S$, guesses if an element $e$ is a duplicate or not.
	\item $\mathsf{Insert}: \mathcal M \times \Gamma \rightarrow \mathcal M$ takes as input the current state, an element $e$ to insert in the filter, and returns an updated memory state.
\end{itemize}

Furthermore, since we work in a context where the memory is too small for perfect detection, we have the assumption that $|\mathsf S| \ll U$.

\begin{definition}[False positive, false negative]
	Let $e$ be an unseen (resp. duplicate) element in the stream $E = \{e_1, \dotsc, e_n\}$, and let $\mathcal F$ be a filter. $e$ is a \emph{false positive} (resp. \emph{false negative}) if $\mathsf{Detect}(e) = \DUPLICATE$ (resp. $\UNSEEN$).
\end{definition}

\begin{definition}[False positive rate, false negative rate]
	Let $E = \{e_1, \dotsc, e_n\}$ be a stream, $\mathcal F$ be a filter. The false positive (resp. false negative) rate of $\mathcal F$ over the first $i < n$ elements, noted $\FPR_i$ (resp. $\FNR_i$), is defined as the number of false positives divided by the number of unseen elements of $\{e_1, \dotsc, e_i\}$ (resp. the number of false negative divided by the number of duplicate elements).
\end{definition}

\subsection{Filter Saturation}
While the previous definition sounds interesting and is often simpler to work with, especially for the derivation of the FPR and FNR, it has been shown that this kind of duplicate detection is an ill-formed problem, as filters saturate and reach the asymptotic relation $\FPR_\infty + \FNR_\infty = 1$, which means that the filter is not any more efficient than a filter answering randomly. We reproduce the statement and proof from \cite{10.1145/3297280.3297335}.

	\begin{theorem}[\cite{10.1145/3297280.3297335}]\label{thm:sat_u}
	Let $\FNR_\infty$ and $\FPR_\infty$ be the asymptotic false negative and false positive rates of a filter of $M$ bits of memory (i.e. the FNR and FPR on a stream of size going to infinity).
	
	If $U \gg M$, then $\FNR_\infty + \FPR_\infty = 1$, which characterises random filters (i.e. filters answering randomly to any query).
\end{theorem}

\begin{proof}
	First note that random filters always verify the relation $\FNR + \FPR = 1$: given that a random filter will return \DUPLICATE with a probability of $p$, an unseen element will be classified as \DUPLICATE with probability $p$, and a duplicate will be classified as \UNSEEN with probability $1-p$, hence the result.
	
	Now, on infinite streams with infinitely many different elements, filters are saturated with information. Given that a filter can only store at most one element per bit (cf. \cite[][Section 2.1]{10.1145/3297280.3297335}), thus a filter of size $M$ can remember at most $M$ elements. However, after $fM$ insertions (with $f \gg 1$), the filter remembers at most a tiny fraction $\frac 1f \ll 1$ of the stream. Having reached its saturated state, the filter has an extremely tiny probability $\frac 1f \approx 0$ of correctly guessing whether the incoming element is a duplicate or not, given what the filter actually knows about the stream. For this reason, the best strategy of a saturated filter is almost indistinguishable from a random strategy, i.e. randomly outputting \DUPLICATE. When the stream grows indefinitely, the filter becomes asymptotically equivalent to a random filter.
\end{proof}

As a consequence, after a sufficient number of elements, no filter can give a significant answer about the duplicate problem.

However, in most practical cases, it is not essential to detect duplicates since the start of the stream, but only duplicates on a sliding window, i.e., duplicates over the last $W$ elements for some integer $W$.

\subsection{Duplicate Detection over a Sliding Window}
In this section, we redefine the problem in order to include a sliding window.

\begin{definition}[Duplicate element, unseen element over a sliding window]
	Let $E = \{e_1, \dotsc, e_n, \dotsc\}$ be a stream over $\Gamma$, let $W\in \mathbb N$ be a sliding window size. $e_j$ is called a \emph{duplicate} in $E$ over the sliding window $W$ if and only if $\exists j - W \leq i < j, e_i = e_j$. Otherwise, $e_j$ is said to be \emph{unseen}.
\end{definition}

One observes that a duplicate or unseen element in $E = \{e_1, \dotsc, e_n\}$ over $W$ is equivalent to a duplicate or unseen element in $E' = \{e_{n-W}, \dotsc, e_n\}$ without a sliding window.

Similarly, we redefine the false positive, false negative, false positive rate and false negative rate.

\begin{definition}[False positive, false negative over a sliding window]
	Let $W$ be the size of a sliding window, $e$ be an unseen (resp. duplicate) element in $E = \{e_1, \dotsc, e_n\}$ over $W$, and let $\mathcal F$ be a filter. $e$ is a \emph{false positive} (resp. \emph{false negative}) over $W$ if $\mathsf{Detect}(e) = \DUPLICATE$ (resp. $\UNSEEN$).
\end{definition}

\begin{definition}[False positive rate, false negative rate over a sliding window]
	Let $E = \{e_1, \dotsc, e_n\}$ be a stream, $\mathcal F$ be a filter, $W$ be the size of a sliding window. The false positive (resp. false negative) rate of $\mathcal F$ over the first $i < n$ elements and the sliding window $W$, noted $\FPR^W_i$ (resp. $\FNR^W_i$), is defined as the number of false positives over $W$ divided by the number of unseen elements in $E$ over $W$ (resp. the number of false negative over $W$ divided by the number of duplicate elements over $W$).
\end{definition}

\subsection{Bounds on the Duplicate Detection over a Sliding Window Problem}

\begin{theorem}
	Let $W$ be a sliding window size, and let us consider a duplicate detection problem over $W$. If the available amount of memory $M$ is such that $M \simeq 2W\log_2W$, then the problem can be solved with almost no error.
\end{theorem}

\begin{proof}
Let us consider a duplicate detection problem of a stream $E$ over a sliding window $W$. We first devise a hash function, whose fingerprints are $2 \log_2 W$ bits long.

The birthday paradox, as summed up in \cite{10.1007/3-540-45708-9_19}, states that for a hash function $H$ over $a$ bits, one must, on average, collect $2^{a/2}$ hashes of different messages before obtaining a collision. Hence, in our case, we can store $2^{2 \log_2 W / 2} = W$ fingerprints of different elements before having a collision, i.e., two distinct elements of the stream $e_i, e_j$ with $i \neq j, e_i \neq e_j$ which have the same hash, i.e. $H(e_i) = H(e_j)$.

Hence, one can construct a filter $\mathcal F$ which consists in a queue of $W$ hashes: for each new element $e$, \textsf{Detect}(e) returns \DUPLICATE if $H(e)$ is present in the queue, \UNSEEN otherwise. \textsf{Insert}(e) just adds $H(e)$ to the queue before popping the list. There is no false negative, and a false positive only happens if two elements have the same hash, which does not happen often, hence a low FPR and a null FNR.

The queue stores $W$ hashes, and as such requires $W \cdot 2\log_2 W$ bits of memory.
\end{proof}

One can also observe that the lookup algorithm on this structure is trivially parallelizable, with a very low error rate, hence having an efficient and near optimal solution.

As such, the problem remains open for cases where $M \ll W \log_2 W$.

\section{Unsuitability of Most Filters for Duplicate Detection over a Sliding Window}
While the duplicate detection problem over a sliding window is only but a slight reformulation of the original problem of duplicate detection, it turns out that most streaming filters which perform well in the latter problem are not as performant in the former.

From our literature reviews, there are 6 competing duplicate filters adapted for streaming data: 
\begin{itemize}
	\item A2 filters \cite{Yoo10}
	\item Stable Bloom Filters (SBF) \cite{Den06}
	\item Streaming Quotient Filters (SQF)\footnote{It has been shown in \cite{10.1145/3297280.3297335} that SQFs are strictly less efficient than QHTs. As such, they will not be used in the rest of this paper.} \cite{Dut13}
	\item Quotient Hash Tables (QHT) \cite{10.1145/3297280.3297335}
	\item Block-decaying Bloom Filters (b\_DBF) \cite{She08}
	\item A variation of Cuckoo Filters \cite{Fan14}, suggested by \cite{10.1145/3297280.3297335}
\end{itemize}
\todo[Regarder https://www2005.org/cdrom/docs/p12.pdf]

All these filters except the b\_DBF were not defined in the context of a sliding window $W$. As a consequence, their parameters do not depend on $W$, and as such, behave poorly when used in the paradigm of duplicate detection over a sliding window.

Experimentally, we observe that the false positive rate of these filters does not decrease to 0 even though the sliding window becomes small; this is because in these filters, 


\section{Queuing Filters for Sliding Window Adaptation}

\section{Application on QHT}

\section{Optimal Error rate on a Filter}

\section{Experiments and Benchmarks}

%\begin{acks}
%
%\end{acks}

%
% The next two lines define the bibliography style to be used, and the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}

% 
% If your work has an appendix, this is the place to put it.
%\appendix

%\section{Research Methods}

\end{document}
