\input{preamble.tex}

\begin{document}

\title{Adapting Duplicate Filters to a Sliding Window Context}

%% Replace by \iffalse when submitting anonymously 
\iftrue 
	\author{Rémi Géraud-Stewart}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \state{France}
	} \email{remi.geraud@ens.fr}

	\author{Marius Lombard-Platet}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \country{France}
	}
	\affiliation{%
	\institution{Be-Studys}
	\city{Geneva} \country{Switzerland}
	} \email{marius.lombard-platet@ens.fr}

	\author{David Naccache}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \country{France}
	} \email{david.naccache@ens.fr}
\fi 

%% Abstract
\begin{abstract}
	A duplicate detection filter is an online data structure designed to efficiently detect repetitions (duplicates) in a stream, when only a limited amount of memory is available. A well-known data structure used in this context is the Bloom filter, which has been generalised in several ways to 
	improve its performances or strike specific tradeoffs. 
	
	When analysed in the limit of an infinite stream, error rates of duplicate filters turn out to be heavily constrained; as a consequence they appear to provide no advantage, asymptotically, over a random yes-no coin toss \cite{10.1145/3297280.3297335}.

	In this paper we consider a \enquote{windowed} variation on the duplicate detection problem
	that is better-behaved and thus lends itself to a more thorough 
	mathematical analysis. We show how existing filters can be adjusted to work
	in this new setting using a queuing construction. 
\end{abstract}

%% CCS XML
\begin{CCSXML}
	% Get code at http://dl.acm.org/ccs.cfm.
	% Please copy and paste the code
\end{CCSXML}

%% Keywords
\keywords{
	% TODO
}

\maketitle

\section{Introduction}
\subsection{Motivation}

We begin with the following, ideal problem: 
\begin{definition}[Duplicate detection problem, DDP] Given a stream $E_n = \{e_1, e_2, \dotsc, e_n\}$ and a \enquote{new} item $e^\star$, find whether $e^\star \in E_n$.\footnote{
	We do not consider here \emph{fuzzy} detection approaches\cite{10.5555/1287369.1287420,1410199,10.1109/ICDE.2011.5767865,10.1109/ICDE.2012.20,Monge97anefficient} that study situations where $e^\star$ belongs to $E_n$ when there exists a \enquote{similar enough} element in the stream.
}
	At every time increment, the new item is added to the stream, i.e., $E_{n+1} = E_n \cup \{e^\star\}$. 
\end{definition}
Instances of this problem abound in computer science, with applications in file system indexation, database queries, network load balancing, network management \cite{10.5555/647912.740658}, in credit card fraud detection \cite{DBLP:journals/corr/abs-1709-08920}, phone calls data mining \cite{10.1145/347090.347094}, etc. A discussion about algorithms on large data streams can be found in \cite{10.1145/776985.776986}. 

In practice, additional constraints exist that we can capture with the following definition:
\begin{definition}[DDP with bounded memory]
	At every time step $n$, given $e^{\star}$ and a current state (dependent on history) of at most $M$ bits, solve the DDP for $E_n$ and $e^{\star}$.
\end{definition}
%Even when the stream is bounded, i.e. $\lim_{n\to\infty}|E_n| < \infty$, it may be impractical to perform a complete lookup and approximate solutions are desirable. 
One can prove \cite[Theorem 2.1]{10.1145/3297280.3297335} that perfect detection of duplicates in a stream with $U$ possible values requires $U$ bits of memory: even for modest situations (e.g. 64-bit nonces, corresponding to $2^{64}$ possible values) this is quickly impractical. As a result, we need a further relaxation of the DDP that allows for errors: 
\begin{definition}[Approximate DDP with bounded memory]
	Solve the DDP using at most $M$ bits of memory, with at most $\epsilon < 1/2$ mispredictions.\footnote{Given a classifier with $\epsilon > 1/2$, we obtain a classifier with $\epsilon < 1/2$ by reversing its output.} 
\end{definition}
Approximate duplicate detection has many real-life use cases, and can sometimes play a critical role, for instance in cryptographic schemes where all security and secrecy fall apart as soon as a random nonce is used twice, such as the ElGamal \cite{10.1007/3-540-39568-7_2} or ECDSA signatures. Other uses include improvements over caches \cite{4484874}, duplicate clicks \cite{10.1145/1060745.1060753}, and others.

Finally, it is clear that the input distribution plays a central role regarding how efficiently the DDP can be solved. For instance, some deterministic streams may be expressed very compactly (such as the output of a PRNG with known seed) making the DDP relatively easy. Information-theoretically, if the source has $U$ bits of entropy then the situation is equivalent to having an $U$-bit, uniformly distributed input.\footnote{Another motivation for this is pragmatic: in many applications, duplicate detection is performed on \emph{hashes} of actual data rather than on the data itself, and we may assume that hashes are uniformly distributed.} 

\todo[continuer].

\subsection{Contributions}

\subsection{Related Work}\label{sub:related}
\todo[continuer]
In \cite{10.1145/1060745.1060753}, the authors explore the behaviour of a structure very close to the counting Bloom filter in some specific sliding window context, that they call landmark- and jumping sliding windows. However, they do not deal with the problematic of a sliding window where at each new element added to the sliding window, another element is removed.
Closer to our work, the idea of using subfilters, tossing old ones has already been explored in the A2 filter \cite{Yoo10}. However, the author only considered a small portion of the problem, as they only discussed about using two subfilters, and only Bloom Filters acting as subfilters.

In this work, we provide a full abstraction of the mechanism, allowing us to use many other DDF than Bloom Filters. We also provide a generalised analysis of the error rates.
\subsection{Organisation}


\section{Duplicate Detection and Filter Saturation}
\subsection{Notations and Initial Problem Statement}
Let us consider an alphabet $\Gamma$. A stream $E$ over $\Gamma$ is a (possibly infinite) set $E = \{e_1, e_2, \dotsc\}$, with each $e_i \in \Gamma$.

\begin{definition}[Duplicate and unseen element]
Let $E = \{e_1, e_2, \dotsc\}$ be a stream over $\Gamma$. Some element $e_j \in E$ is called a \emph{duplicate} if and only if there exists $i < j$ such that $e_i = e_j$. In that case we write $e_j \dup E$. Otherwise, $e_j$ is said to be \emph{unseen} and we write $e_j \notdup E$.
\end{definition}

\begin{definition}[Duplicate detection filter]
	A \emph{duplicate detection filter} (DDF) is a collection 
	$\mathcal F = (\mathcal S, \mathsf{Insert}, \mathsf{Detect})$ of a state\footnote{Here, $\mathcal M$ denotes all the memory states that can be reached by the filter.} $\mathcal S \in \mathcal M$ and two algorithms,
	\begin{itemize}
		\item $\mathsf{Detect}: \mathcal M \times \Gamma \to \{\DUPLICATE, \UNSEEN\}$;
		\item $\mathsf{Insert}: \mathcal M \times \Gamma \to \mathcal M$.
	\end{itemize}
	Informally, the $\mathsf{Detect}$ algorithm tentatively labels an element $e$ as duplicate or not, given the current state $\mathcal S$; whereas $\mathsf{Insert}$ takes as input the current state, an element $e$ to insert in the filter, and returns an updated memory state $\mathcal S'$. 
\end{definition}

We will usually consider the situations where the available memory is too small for perfect detection, i.e., $|\mathcal S| \ll |\Gamma|$.

\begin{definition}[False positive, false negative]
	Let $e \dup E$ (resp. $e \notdup E$) and $\mathcal F$ be a filter. We say that $e$ is a \emph{false positive} (resp. \emph{false negative}) when $\mathcal F.\mathsf{Detect}(e) = \DUPLICATE$ (resp. $\UNSEEN$).
\end{definition}

\begin{definition}[False positive rate, false negative rate]
	Let $E$ be a stream, $\mathcal F$ be a filter. The \emph{false positive rate} (resp. \emph{false negative rate}) of $\mathcal F$ over the first $i < n$ elements, denoted $\FPR_i$ (resp. $\FNR_i$), is defined as the ratio of false positives divided over the number of unseen elements in $\{e_1, \dotsc, e_i\}\subset E$ (resp. the ratio of false negative divided by the number of duplicate elements in that subset).
\end{definition}

However, it is more practical to work with another value, namely, the probability that after $m$ insertions, the element $e^\star$ is a false positive (resp. false negative). 

\begin{definition}[Probability of false positive, of false negative]
	Let $E = \{e_1, \dotsc\}$ be a stream, $\mathcal F$ be a filter. The \emph{probability of false positive} (resp. \emph{probability of false negative}) of $\mathcal F$ after $i$ elements, denoted $\FP_i$ (resp. $\FN_i$), is defined as the probability that $e_{i+1}$ is a false positive (resp. false negative).
\end{definition}

One can observe that $\FPR_n = \frac{1}{n}\sum_{i=1}^m \FP_i$, and similarly for $\FNR_n$.

\subsection{Filter Saturation and Sliding Windows}
Unfortunately, efficient duplicate detection over an infinite stream has been identified as an unsolvable problem \cite{10.1145/3297280.3297335}. As a matter of fact, as the number of bits of information of the stream grow to infinity, the filter can only remember an ever diminishing proportion of those bits, eventually having no advantage compared to a filter answering randomly.

In other terms, any DDF is asymptotically as good as a random guess:
	\begin{theorem}[\cite{10.1145/3297280.3297335}]\label{thm:sat_u}
	Let $\FNR_\infty$ and $\FPR_\infty$ be the asymptotic false negative and false positive rates of a filter of $M$ bits of memory (i.e. the FNR and FPR on a stream of size going to infinity).
	
	If $|\Gamma| \gg M$, then $\FNR_\infty + \FPR_\infty = 1$, which characterises random filters (i.e. filters answering randomly to any query).
\end{theorem}

One way out of this situation is to lower our expectations and work on a sliding window:

\begin{definition}[Sliding window DDP]
	Given a stream $E_n$ of fixed size $w$, with $E_n = \{e_{n-w+1}, \dotsc, e_n\}$ and an item $e^\star$, find whether $e^\star \in E_n$.
\end{definition} 

We therefore introduce new definitions suited to that setting:

\begin{definition}[Duplicate element, unseen element over a sliding window]
	Let $E = \{e_1, e_2, \dotsc\}$ be a stream over $\Gamma$, let $w\in \mathbb N$, an element $e_j \in E$ is called a \emph{duplicate} in $E$ over the sliding window of size $w$ if and only if there exists $j$ such that $j - w \leq i < j$  and $e_i = e_j$. In this case we write $e_j \wdup E$. Otherwise, $e_j$ is said to be \emph{unseen} and we write $e_j \notwdup E$.
\end{definition}

\begin{definition}[False positive, false negative over a sliding window]
	Let $w \in \mathbb N$, $e \notwdup E$ (resp. $e \wdup E$) and let $\mathcal F$ be a DDF. We say that $e$ is a \emph{false positive} (resp. \emph{false negative}) over $w$ if $\mathsf{Detect}(e) = \DUPLICATE$ (resp. $\UNSEEN$).
\end{definition}

\begin{definition}[False positive rate, false negative rate over a sliding window]
	Let $E$ be a stream, $\mathcal F$ be a DDF, $w \in \mathbb N$. The \emph{false positive} (resp. \emph{false negative}) rate of $\mathcal F$ over the first $i < n$ elements and the sliding window $w$, noted $\FPR^w_i$ (resp. $\FNR^w_i$), is defined as the number of false positives over $w$ divided by the number of unseen elements in $E$ over $w$ (resp. the number of false negative over $w$ divided by the number of duplicate elements over $w$).
\end{definition}


\subsection{Bounds on the Sliding Window DDP}\label{sub:opt_solutions}

Our goal is to minimise both the false positive and false negative rates of a DDF, given a fixed and limited amount of memory. Before going into details, we first explore the limits of the problem, i.e., when perfect or very performant solutions exist.

\begin{theorem}
	For $M \geq w (\log_2(w) + 1) \log_2(|\Gamma|)$, the sliding window DDP can be solved exactly (with no errors) in constant time.
\end{theorem}

\begin{proof}
	We explicitly construct a DDF that performs the detection. Storing all $w$ elements in the sliding window takes $w\log_2(|\Gamma|)$ memory, using a FIFO queue $Q$; however 
	lookup has a worst-time complexity of $O(w)$. 
	
	We therefore rely on an
	ancillary data structure for the sake of quickly answering lookup questions.
	Namely we use a dictionary $D$ whose keys are elements from $\Gamma$ and values are counters.
	
	Whe an element $e$ is inserted in the DDF, $e$ is stored and $D[e]$ is incremented (if the key $e$ did not exist in $D$, it is created first, and
	$D[e]$ is set to $1$). In order to keep the number of stored elements to $w$, 
	we discard the oldest element $e_\text{last}$ in $Q$. As we do so, we also decrement $D[e_\text{last}]$, and if $D[e_{last}] = 0$ the key is deleted from $D$. The whole insertion procedure is therefore performed in constant time.
	
	Lookup of an element $e^\star$ is simply done by looking whether the key $D[e^\star]$ exists, which is done in constant time.
	
	The size of the queue is $w\log_2 |\Gamma|$, the size of the dictionary is $w \log_2 |\Gamma| \log_2 w$ (as the dictionary cannot have more than $w$ keys as the same time, and a counter cannot go over $w$, thus being less than $\log_2 w$ bits long). Thus at most $w (\log_2(w) + 1) \log_2(|\Gamma|)$ bits are necessary for this DDF to work.

	Finally this filter does not make any mistake, as every positive has a witness in $Q$ and any negative is absent from the last $w$ elements.
\end{proof}
The dependence on $\log_2 |\Gamma|$ can be droped, at the cost of allowing 
errors:
\begin{theorem}
	Let $w \in \mathbb N$. Let $M \simeq 2w\log_2w$, then the sliding window DDP can be solved with almost no error using $M$ bits of memory.
	
	More precisely, it is possible to create a filter of $M$ bits with an FNR of $0$, and an FPR of $1 - (1-\frac1{w^2})^w \sim \frac 1w$, and a time complexity of $O(w)$.
	
	Using $M \simeq 5w\log_2 w$ bits of memory, a constant-time filter with the same error rate can be constructed.
\end{theorem}

\begin{proof}
	Here again we explicitely construct the filters that achieve the theorem's bounds.

	Let $h$ be a hash function with codomain $\{0,1\}^{2 \log_2 w}$.
The birthday theorem, as summed up in \cite{10.1007/3-540-45708-9_19}, states that for a hash function $h$ over $a$ bits, one must on average collect $2^{a/2}$ input-output pairs before obtaining a collision. Therefore $2^{(2 \log_2 w) / 2} = w$ hash values $h(e_i)$ can be computed before having a $50\%$ probability of a collision (here, a collision is when two distinct elements of the stream $e_i, e_j$ with $i \neq j, e_i \neq e_j$ have the same hash, i.e. $h(e_i) = h(e_j)$). The 50\% threshold we impose on $h$ is arbitrary but nonetheless practical.

Let $\mathcal F$ be the following DDF: the filter's state consists in a queue of $w$ hashes, and for each new element $e$, $\mathsf{Detect}(e)$ returns \DUPLICATE if $h(e)$ is present in the queue, \UNSEEN otherwise. $\mathsf{Insert}(e)$ appends $h(e)$ to the queue before popping the queue. 

There is no false negative, and a false positive only happens if the new element to be inserted collides with at least one other element, which happens with probability $1 - (1 - \frac 1{2^{2\log_2w}})^{w} = 1 - (1-\frac1{w^2})^w$, hence an FNR of $0$ and a FPR of $1 - (1-\frac1{w^2})^w$.
The queue stores $w$ hashes, and as such requires $w \cdot 2\log_2 w$ bits of memory.

Note that this solution has a time complexity of $O(w)$. Using an additional dictionary, as in the previous proof, we get a filter with an error rate of about $\frac 1w$ and constant time for insertion and lookup, using $5w\log_2 w$ bits of memory.
\end{proof}

When $\log_2|\Gamma| > 2 \log_2 w$ (which is the case of most practical use cases) this DDF outperforms the naïve strategy\footnote{The naïve strategy consisting of storing the $w$ elements of the sliding window, requiring $w \log_2|\Gamma|$ bits of memory.}, both in terms of time and memory.


Hence, very good solutions for situations where $M \approx 5w\log_2 w$ already exist, but efficient solutions and lower bounds on the error rates for smaller amounts of memory are still to be found.


\subsection{DDP vs Sliding Window DDP}
The sliding window variant of DDP may seem to be a slight weakening of the original problem. Remarkably, it turns out that DDFs which perform well for the DDP do rather poorly for the sliding window DDP. A literature review collects the following DDF constructions: A2 filters \cite{Yoo10}, Stable Bloom Filters (SBF) \cite{Den06}, Quotient Hash Tables (QHT) \cite{10.1145/3297280.3297335}, Streaming Quotient Filters (SQF) \cite{Dut13}, Block-decaying Bloom Filters (b\_DBF) \cite{She08}, and a slight variation of Cuckoo Filters \cite{Fan14} suggested by \cite{10.1145/3297280.3297335}. The structure proposed in \cite{10.1145/1060745.1060753} is not suited to either sliding window or no sliding window DDP, as they work on what they call `landmark` sliding window, which consists of a zero-resetting of the memory at some user-defined epochs.

Of these, only the A2 and b\_DBF were designed in a way immediately applicable to the sliding window DDP. The others do not account for a finite-length sliding window, and therefore cannot be adjusted as a function of $w$. Another remarkable (but experimental) observation is that these DDFs' false positive rate does not decrease to 0 when the sliding window becomes small.

\todo[Expliquer ?]


\section{Queuing filters}
In this section, we describe the queuing construction, which produces a sliding window DDF from any DDF. We first give the description of the setup, before studying the theoretical error rates. 
A scheme describing our structure is detailed in \ref{fig:scheme}.

\subsection{The queueing construction}

\paragraph{Principle of operation.}
Let $\mathcal F$ be a DDF. Rather than allocating the whole memory to $\mathcal F$, we will create $L$ copies of $\mathcal F$, each using a fraction of the available memory. Each of these \emph{subfilters} as a limited timespan, and is allowed up to $c$ insertions. The subfilters are organised in a queue.

When inserting a new element in the queueing filter, it is inserted in the topmost subfilter of the queue. After $c$ insertions, a new empty filter is added to the queue, and the oldest subfilter is popped and erased.

As such, we can consider that each subfilter operates on a sub-sliding window of size $c$, which makes the overall construction a DDF operating over a sliding window of size $w = cL$.

\begin{figure}
	\input{fig/queuef.tex}
	\caption{Architecture of the queuing filter. The filter is composed of $L$ subfilters $\mathcal F$, each containing up to $c$ elements. Once the newest subfilter has inserted $c$ elements in its structure, the oldest one is expired. As such, it is dropped and a new one is created and put under population at the beginning of the queue. In this example, the sub-sliding window of $\mathcal F_1$ is $\{e_{m-2}, e_{m-3}, e_{m-4}\}$.} \label{fig:scheme}
\end{figure}

\paragraph{Insertion and lookup.}
The filter returns \DUPLICATE if and only if at least one subfilter does. Insertion is a simple insertion in the topmost subfilter.

\paragraph{Queue update.}
After $c$ insertions, the last filter of the queue is dropped, and a new (empty) filter is appended in front of the queue.

\paragraph{Pseudocode.}
We give a brief pseudo-code of the queuing filter's functions \textsf{Lookup} and \textsf{Insert}, as well as a \textsf{Setup} function for initialisation, in \ref{alg:queueing}. We introduced for simplicity a constructor $\mathcal F.\textsf{Setup}$ that takes as input an integer $M$ and outputs an initialized empty filter $\mathcal F$ of size at most $M$. The \texttt{subfilter} is a FIFO that has a \texttt{pop} and \texttt{push\_first} operations, which respectevely removes the last element in the queue or inserts a new item in first  position.   

\begin{algorithm}[]
	\caption{Queueing Filter Setup, Lookup and Insert}\label{alg:queueing}
	
	\begin{algorithmic}[1]
	\Function{Setup}{$\mathcal F, M, L, c$}
		\Comment $M$ is the available memory, $\mathcal F$ the subfilter structure, $L$ the number of subfilters and $c$ the number of insertions per subfilter
		\State subfilters $\gets \emptyset$
		\State counter $\gets 0$
		\State $m \gets \lfloor M/L \rfloor$
		\For{$i$ from $0$ to $L-1$}
			\State subfilters.push\_first$(\mathcal F.\mathsf{Setup}(m))$
		\EndFor
		\State \textbf{store} (subfilters, $L$, $m$, counter)
		\State\Return filter
	\EndFunction 
	\item[]
	\end{algorithmic}

	\begin{algorithmic}[1]
	\Function{Lookup}{$e$}
		\For{$i$ from $0$ to $L-1$}
			\If{subfilter[i]$.\mathsf{Lookup(e)}$}
				\State \Return $\DUPLICATE$
			\EndIf
		\EndFor
		\State \Return \UNSEEN
	\EndFunction
	\item[]
	\end{algorithmic}

	\begin{algorithmic}[1]
	\Function{Insert}{$e$}
		\State subfilters[0]$.\mathsf{Insert}(e)$
		\State counter $\gets$ counter + 1 
		\If{counter == $c$}
			\State subfilters.pop()
			\State subfilters.push\_first($\mathcal F.\textsf{Setup}(m)$)
		\EndIf
	\EndFunction
	\end{algorithmic}
\end{algorithm}

\subsection{Error Rate Analysis}
Depending on the underlying subfilter, the queueing filter will behave differently. However, its false positive and false negative probabilities can be derived from the underlying subfilter's false positive and false negative probabilities. 

\subsubsection{Number of Elements in a Queueing Filter}\label{subsub:number_elements}
Before deriving the error rates, it is important to analyse how many elements are in the filter. We know, by design, that no more than $cL$ elements can be present. However, after exactly $cL$ insertions, the last subfilter is deleted, to be replaced by an empty one at the beginning of the queue. Hence, after $cL$ insertions, only $c(L-1)$ elements are stored in the queueing filter.

Hence, the number of elements in the filter oscillate between $c(L-1)$ and $cL$. Furthermore, we have $cL \approx w$, which means that the number of elements in the filter can be higher than $w$. These facts must be kept in mind while deriving the error rates.

\subsubsection{Probability of False Positive}
\begin{theorem}
	Let $\mathcal Q$ be a queueing filter, consisting of $L$ underlying subfilters $\mathcal F$, each filled with up to $c$ elements. Let $\FP_{\mathcal F,m}$ be the probability of false positive of the subfilter $\mathcal F$ after $m$ insertions, let $\FP_{\mathcal Q, m}^w$ be the probability of false positive of $\mathcal Q$ after $m$ insertions on a sliding window of size $w$.
	
	We have $$\FP_{\mathcal Q, m}^w = 1 - (1 - \FP_{\mathcal F,c})^{L-1}(1 - \FP_{\mathcal F, m \% c })$$ where \% is the modulo operator.
\end{theorem}

\begin{proof}
	Let $E = \{e_1, \dotsc, e_m, \dotsc\}$ be a stream, let $w$ be a sliding window and let $e^\star \notwdup E$. We first take into account the remark in \ref{subsub:number_elements} that the filter $\mathcal Q$ may contain more than $w$ elements, and hence potentially contain $e$ in memory, in the last subfilter of the queue. However, such event is quite rare, and can be neglected: let $\delta = cL - w$ the number of additional elements that can be stored in $\mathcal Q$. The probability that $e^\star$ has occurred in $\{e_{m-cL}, \dotsc e_{m-w+1}\}$ is $1-\left(1-\frac1{|\Gamma|}\right)^\delta \approx \frac{\delta}{|\Gamma|} \approx 0$ since $\delta$ is likely to be small, while $|\Gamma|$ is too big for being stored in memory.
	
	Hence, we can neglect the probability that $e^\star$ is present in the filter. Therefore, $e^\star$ is a false positive if and only if at least one subquery $\mathcal F_i.\mathsf{Lookup(e^\star)}$ returns \DUPLICATE. Conversely, $e^\star$ is not a false positive if and only if all subqueries $\mathcal F_i.\mathsf{Lookup(e^\star)}$ return \UNSEEN. This can be rewritten as $e^\star$ is not a false positive if and only if $e^\star$ is not a false positive for each subfilter.
	
	Given that each subfilter stores $c$ elements, safe for the first subfilter which only stores $m \% c$ elements, we get
	$$\FP_{\mathcal Q, m}^w = 1 - (1 - \FP_{\mathcal F, m \% c })(1 - \FP_{\mathcal F,c})^{L-1}$$
\end{proof}

\subsubsection{Probability of False Negative}\label{sub:fnr}
\begin{theorem}
	Let $\mathcal Q$ be a queueing filter, consisting of $L$ underlying subfilters $\mathcal F$, each filled with up to $c$ elements. Let $\FN_{\mathcal F,m}$ be the probability of false negative of the subfilter $\mathcal F$ after $m$ insertions, let $\FN_{\mathcal Q, m}^w$ be the probability of false negative of $\mathcal Q$ after $m$ insertions on a sliding window of size $w$.
	
	For $p_m = \frac{1-\left(1-\frac 1{|\Gamma|}\right)^m}{1-\left(1-\frac 1{|\Gamma|}\right)^w}$, we have 
	\begin{align*}
	\FN_{\mathcal Q, m}^w =& \left[p_c\FN_{\mathcal F,c} + \left(1-p_c\right)\left(1-\FP_{\mathcal F,c}\right)\right]^{L-1}
	\\&\qquad
	 \cdot \left[p_{m\%c}\FN_{\mathcal F,m\%c} + \left(1-p_{m\%c}\right)\left(1-\FP_{\mathcal F,m\%c}\right)\right]
	\end{align*}
	
	where $\%$ is the modulo operator.
\end{theorem}

\begin{proof}
	Let $E = \{e_1, \dotsc, e_m, \dotsc\}$ be a stream, let $w$ be a sliding window and let $e^\star \wdup E$.
	We first observe that if $cL > w$, there might be some duplicates of $e^\star$ that happened before the current sliding window, but might still be remembered by the oldest filter. However, as argued in the previous section, the probability of such an event is negligible.
	
	Hence, we consider that the only duplicates are part of the sliding window. $e^\star$ is a false negative if and only if all subfilters $\mathcal F_i$ answer $\mathcal F_i.\mathsf{Detect}(e^\star) = \UNSEEN$.
	
	There can be two cases:
	\begin{itemize}
		\item $e^\star$ is present in $\mathcal F_i$'s sub-sliding window
		\item $e^\star$ is not present in $\mathcal F_i$'s sub-sliding window
	\end{itemize}

In the first case, $\mathcal F_i.\mathsf{Detect}(e^\star)$ returns \UNSEEN if and only if $e^\star$ is a false negative for $\mathcal F_i$. Hence, in this case, the probability of $\mathcal F_i$ to return \UNSEEN is $\FN_{\mathcal F,c}$. For the subfilter $\mathcal F_0$, however, the probability is $\FN_{\mathcal F, m\%c}$.

In the second case, $\mathcal F_i.\mathsf{Detect}(e^\star)$ returns \UNSEEN if and only if $e^\star$ is not a false positive for $\mathcal F_i$. Hence, in this case, the probability of $\mathcal F_i$ to return \UNSEEN is $1-\FP_{\mathcal F,c}$. For the subfilter $\mathcal F_0$, however, the probability is $1-\FP_{\mathcal F, m\%c}$.

Now, the probability $p_c$ for $e^\star$ to be in subfilter $\mathcal F_i$ (which has had $c$ insertions) sub-sliding window is 

\begin{align*}
	p_c =& Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window | } e^\star \wdup E]\\
	=& \frac{Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window } \cap e^\star \wdup E]}{Pr[e^\star \wdup E]}\\
	=& \frac{Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window}]}{Pr[e^\star \wdup E]}\\
	=& \frac{1 - Pr[\text{$e^\star$ is not in }\mathcal F_i \text{ sub-sliding window }]}{1 - Pr[e^\star \notwdup E]}\\
	p_c=& \frac{1 - \left(1 - \frac 1{|\Gamma|}\right)^c}{1 - \left(1-\frac{1}{|\Gamma|}\right)^w}
\end{align*}

Finally, we get that the probability of a false positive for $\mathcal Q$ over the sliding window $w$ is 
\begin{align*}
\FN^w_{\mathcal Q, m} =& \left[p_c\FN_{\mathcal F,c} + (1-p_c)(1-\FP_{\mathcal F,c})\right]^{L-1}
\\&\quad
\cdot \left[p_{m\%c}\FN_{\mathcal F,m\%c} + (1-p_{m\%c})(1-\FP_{\mathcal F,m\%c})\right]\\
\end{align*}
\end{proof}

This probability is quite complicated and does not allow, in the general case, for an in-depth analysis of the probability of false negative.

\begin{theorem}
	In first approximation, a low value of $L$ leads to a higher false negative probability for the queueing filter.
\end{theorem}

\begin{proof}
As a matter of fact, on average the queueing filter consists of $L-1$ subfilters with $c$ insertions, and one subfilter with $\frac c2$ insertions. As such, we can say that the queueing filter is a representation of the last $(c-\frac12) L$ last elements. 

Let us consider a duplicate $e^\star \wdup E$. If, in the sliding window, all duplicates only happened at least $(c-\frac12)L$ epochs ago (which happens with probability $\left(1-\frac1{|\Gamma|}\right)^{(c-\frac12)L}$), we know that no duplicates happened in any subfilter's sliding window. By using the same reasoning as the theorem's proof, we get a probability of false negative of around $\left[0 + \left(1-\frac1{|\Gamma|}\right)^{(c-\frac12)L}(1-\FP_{\mathcal F,c})\right]^L$. Given that $cL \approx w$ which is a constant, we get that the probability of false positive mostly varies, in this case, with $\left(1-\FP_{\mathcal F,c}\right)^L$. 

Hence, the lower $L$ is, the higher the probability of false negative will be.

The approximations made in this proof are backed by experimental results.
\end{proof}


\section{Application on Existing DDFs}
As our queuing filter relies on another DDF subfilter, we must pick one existing DDF from the literature.
\subsection{DDF Subfilter Selection Strategy}
As a matter of fact, it is preferable to use only one kind of DDF as subfilter, as proven by the following theorem.

\begin{theorem}
	An optimal queueing filter only uses one kind of subfilter.
\end{theorem}

\begin{proof}
	One could imagine a queueing filter in which subfilters would alternate between, say, subfilter of type $A$ and of type $B$. However, we show with a simple symmetry argument that this kind of queueing filter is sub-optimal.
	
	Let us consider a queueing filter relying on several kinds of subfilters. Given that all subfilters play the same role, and that their efficiency is independent of the other subfilters present, we know that one less efficient subfilter can be replaced by a more efficient one, directly improving the efficiency of the queueing filter.
\end{proof}

However, making a theoretical analysis of all DDFs error rates may be tedious. Instead, we propose a ranking on existing DDFs. As \cite{10.1145/3297280.3297335} mentioned, filters all reach a saturation state. The idea is then to order DDFs by their resistance to saturation --i.e., by ranking them by the time they take to reach saturation.

Before ranking the DDFs by the error rate, we first introduce a lower bound on said rate. 

Note: given we are working on selecting the best subfilter for our queueing filter, in this section we are \emph{not} working over a sliding window.

\subsection{Optimal Error Probability on a DDF}\label{sub:optimal_ddf}
    \begin{theorem}
	Let $E$ be a stream of $n$ elements uniformly selected from an alphabet of size $|\Gamma|$. For any DDF of size $M$, the error probability $ER_n$ (consisting of the sum of the FP and FN probabilities) is such that
	$$ER_n \geq 1 - \frac{1 - \left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}$$ for any $n > M$.
	
	Especially, for any filter of size $M$, the asymptotic error rate $ER_\infty$ is such that $ER_\infty \geq \left(1 - \frac{1}{|\Gamma|}\right)^{M}$.
\end{theorem}
We remind that in this section, we are working on DDP over the whole stream and not over a sliding window.

\begin{proof}
	According to classical results on information theory, a perfect filter must use at least $1$ bit to remember one element from a random uniform source with no possible error.
	
	Since the filter has $M$ bits of memory, we conclude that a perfect filter can store at most $M$ elements in memory. Assume that such a filter exists. Because the stream is random, we can assume without loss of generality that the filter stores the M last elements of the stream: any other strategy cannot yield a lower error rate, given the stream is random.
	
	Let us derive the error rate of such a filter.
	If an element is already stored in the filter, then the optimal filter will necessarily answer \DUPLICATE. On the other hand, if the element is not in memory, a perfect filter can choose to answer randomly. Let $p$ be the probability that a filter answers \DUPLICATE when an element is not in memory. An optimal filter will lower the error rate of any filter using the same strategy with a different probability.
	
	An unseen element, by definition, will be unseen in the $M$ last elements of the stream, and hence will not be in the filter's memory, so the filter will return \UNSEEN with probability $1-p$. For this reason, this filter has an FP probability of $p$.
	
	On the other hand, a duplicate element $e^\star \dup E$ is classified as \UNSEEN if and only if it was not seen in the last $M$ elements of the stream, and the filter answers \UNSEEN. Let $D$ be the event "\textsf{There is at least one duplicate in the stream} and $C$ be the event \textsf{There is a duplicate of $e^\star$ in the $M$ previous elements of the stream}.
	
	We have that $e^\star$ triggers a false negative with probability
	
	\begin{align*}
	\FN_n &= (1 - P[C | D])(1-p) =  \left(1 - \frac{P[C \cap D]}{P[D]}\right)(1-p)\\
	& = \left(1 - \frac{P[C]}{P[D]}\right)(1-p) =  \left(1 - \frac{1 - P[\bar C]}{1 - P[\bar D]}\right)(1-p) \\
	\FN_n &=  \left(1 - \frac{1 - \left(1 - \frac 1{|\Gamma|}\right)^M}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}\right)(1-p)
	\end{align*}
	
	Hence, the error probability of the filter is $ER_n = \FN_n + p =  \left(1 - \frac{1 - \left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}\right)(1-p) + p =$ $ 1 - \frac{1-\left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}(1-p)$, which is minimized when $p = 0$.
	
	Given that, by definition, a perfect filter has the lowest error rate of any filter, we get the desired result.
	\end{proof}

Note, as highlighted in the proof, that this bound is not tight, and that better bounds may exist. Also note that this bound only applies when the stream is random: when patterns are found in the stream, information theory informs that data may be stored more efficiently.

As such, the best filter will be the one the closest to this error rate.

\subsection{DDF Comparison}
A filter comparison was already made in \cite{10.1145/3297280.3297335}. Taking the same filters parameters, created another benchmark, studying the resistance of each DDF to saturation. We added the lower bound on the error rate. More details about the experiments are given in \ref{sub:saturation}. We observe in \ref{fig:graph_n} that the two most efficient filters are the QHT \cite{10.1145/3297280.3297335} and the A2 \cite{Yoo10}.

However, as noted in \ref{sub:related}, A2 filters can be considered as a queuing filter, with $L=2$ and Bloom filters as subfilters. Hence, as it is not interesting to put a queueing filter inside a queueing filter, we will rather use QHT for practical benchmarking of the queueing filter.

\subsection{Parameter Tuning}
The parameters of a queueing filter are: the parameters of the subfilter, the number $L$ of subfilters and the number $c$ of insertions per subfilter. 

However, the goal here is not to modify the parameters of the underlying subfilter, but rather to tune the queueing filter so the subfilter can perform as well as possible. Hence, the queueing filter's parameters are only $c$ and $L$.

Furthermore, we have the relation $cL \approx w$, so there is only one free parameter. 

As such, an optimal parameter tuning would be to set $L$ so that the error rate (defined as the sum of the FPR and FNR) would be minimal. Nonetheless, given the complicated nature of the FPR and FNR formulae, deriving an optimal $L$ is out of reach.

We have observed in \ref{sub:fnr} that the FN is positively correlated with
$\left(1-\FP_{\mathcal F,c}\right)^L$, however the FP is negatively correlated with the same value. As such, decreasing the probability of false positive leads to an increase in the probability of false negatives. 

\todo[FAUX, peut etre log ? However, practical tests show (see \ref{sub:L_bench}) that setting $L = \sqrt w$ leads to optimal results.]

%Hence, the (heuristic) optimal solution is to set $L = \sqrt w$.


\section{Experiments and Benchmarks}
In this section, we present the different experiments we made. The code is accessible online and will be disclosed after peer review.

We first present a comparison of the resistance of various DDFs against saturation, without any sliding window. This comparison helps us to choose the best subfilter.

Then, we design an experiment in which we compare the error rates of a QHT and a queuing filter, using the same QHT as a subfilter.

We remind the reader that for small sliding windows compared to the memory size, optimal or nearly optimal solutions can be implemented, as discussed in \ref{sub:opt_solutions}. However, since we must evaluate the number of duplicates in the streams we use, we cannot benchmark our filter in real-life situations, as doing so would require way too much memory for keeping track of the duplicates.

\subsection{Saturation Resistance of DDFs}\label{sub:saturation}
In this section, we compare the resistance of various filters to saturation. From the streaming filters we found in the literature, we only considered the filters not designed for operating on a sliding window, and parameters as \cite{10.1145/3297280.3297335}, namely:

\begin{itemize}
	\item QHT \cite{10.1145/3297280.3297335}, 1 bucket per row, 3 bits per fingerprint.
	\item SQF \cite{Dut13}, 1 bucket per row, $r = 2$ and $r' = 1$
	\item Cuckoo Filter \cite{Fan14}, cells containing 1 element of 3 bits each
	\item Stable Bloom Filter (SBF) \cite{Den06}, 2 bits per cell, 2 hash functions, targeted FPR of 0.02
%	\item $A2$ Filter \cite{Yoo10}, targeted FPR of $0.1$ on the sliding window.
%	\item Block-Decaying Bloom Filter (b\_DBF) \cite{She08}, sliding window of 6000 elements.
\end{itemize}

The stream was a stream of uniform random elements, taken from an alphabet of $2^{26}$ elements, and amounts for about 8\% duplicates on the 150 000 000 elements in the longest stream used. Results are plotted in \ref{fig:graph_n}.

\begin{figure}
	\input{er_n_artificial}
	\caption{Error rate (times 100) of DDFs of 1Mb, depending on the size of the stream, without any sliding window. Hatched area represents over-optimal (impossible) values.}\label{fig:graph_n}
\end{figure}

We observe that the best filters are, in order, QHT, SQF, Cuckoo and SBF. We also observe that QHT and SQF have error rates relatively close to the lower bound, hence suggesting that these filters are close to optimality.

\subsection{Benchmarking Various Queuing Filters}
Now that DDF are compared on a full stream, we observe how each of them behave when they act as a subfilter in our queueing scheme. The goal is to empirically prove the assumption made in \ref{sub:optimal_ddf}, stating that the best (error-wise) DDF operating on a full stream leads to the best (error-wise) queueing filter operating on a sliding window. From \ref{fig:graph_n}, we observe that the best DDF for DDP on a full stream are, in order, QHT, SQF, Cuckoo, SQF.

For this, we used four different queueing filters, with $L = 10$ subfilters each, and with the following subfilters: 
\begin{itemize}
	\item QHT subfilter \cite{10.1145/3297280.3297335}, 1 bucket per row, 3 bits per fingerprint. 
	\item Cuckoo Filter \cite{Fan14}, adapted for streaming as recommanded by \cite{10.1145/3297280.3297335}, cells containing 1 element of 3 bits each
	\item SQF \cite{Dut13}, 1 bucket per row, $r = 2$ and $r' = 1$
	\item SBF \cite{Den06}, 2 bits per cell, 2 hash functions, targeted FPR of 0.02
\end{itemize}

We used a random stream of $1,000,000$ elements uniformly selected from an alphabet of size $|\Gamma| = 2^{16}$, a memory size $M=100,000$ bits, a sliding window of size $w = 10,000$. The error rates (defined as the sum of $\FNR^w$ and $\FPR^w$) are displayed in \ref{tab:comparing_queueing}. We see that in all cases, the QHT subfilter performs better or almost as well than any other subfilter, hence validating the heuristic assumption.

\begin{table}[]
	\begin{tabular}{rccccc}
			   & $w=1000$ & $w=5000$ & $w=10^4$ & $w=10^5$ & $w=10^6$ \\ \hline
		QHT    & \textbf{9.84} 	  & \textbf{26.25}	 & \textbf{41.09}	 & \textbf{87.89}	  & 70.76      \\
		Cuckoo & 84.99	  & 99.94	 & 99.95	 & 99.93	  & 99.80      \\
		SQF    & 16.73 	  & 48.76	 & 69.25	 & 95.77 	  & \textbf{70.14}     \\
		SBF    & 54.88 	  & 93.93	 & 97.33	 & 97.41	  & 71.17      \\
	\end{tabular}
	\caption{Error rate (times 100) of various queueing filters on various sliding window sizes, $M=100000$, $L=10$, $|\Gamma| = 2^{16}$. For each sliding window, the best performance is written in bold.}\label{tab:comparing_queueing}
\end{table}

\todo

\subsection{Benchmark of the influence of $L$ parameter}\label{sub:L_bench}
In this section, we make experiments on the influence of $L$ on the error rate of the queueing filter. 
\subsection{Comparison of QHT and QHT-Queueing Filter}
In this section, we compare the efficiency of QHT and QHT-queueing filter. For this, we use both an artificial dataset, as well as a real one, on various sliding window sizes.




\section{Security}
%% Acknowledgements 
%\begin{acks} \end{acks}

%% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{qhtv2}

%% Appendix
%\appendix

\end{document}
