\input{preamble.tex}

\begin{document}

\title{Adapting Duplicate Filters to a Sliding Window Context}

%% Replace by \iffalse when submitting anonymously 
\iftrue 
	\author{Rémi Géraud-Stewart}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \state{France}
	} \email{remi.geraud@ens.fr}

	\author{Marius Lombard-Platet}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \country{France}
	}
	\affiliation{%
	\institution{Be-Studys}
	\city{Geneva} \country{Switzerland}
	} \email{marius.lombard-platet@ens.fr}

	\author{David Naccache}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \country{France}
	} \email{david.naccache@ens.fr}
\fi 

%% Abstract
\begin{abstract}
	A duplicate detection filter is an online data structure designed to efficiently detect repetitions (duplicates) in a stream, when only a limited amount of memory is available. A well-known data structure used in this context is the Bloom filter, which has been generalised in several ways to 
	improve its performances or strike specific tradeoffs. 
	
	When analysed in the limit of an infinite stream, error rates of duplicate filters turn out to be heavily constrained; as a consequence they appear to provide no advantage, asymptotically, over a random yes-no coin toss \cite{10.1145/3297280.3297335}.

	In this paper we consider a \enquote{windowed} variation on the duplicate detection problem
	that is better-behaved and thus lends itself to a more thorough 
	mathematical analysis. We show how existing filters can be adjusted to work
	in this new setting using a queuing construction. 
\end{abstract}

%% CCS XML
\begin{CCSXML}
	% Get code at http://dl.acm.org/ccs.cfm.
	% Please copy and paste the code
\end{CCSXML}

%% Keywords
\keywords{
	% TODO
}

\maketitle

\section{Introduction}
\subsection{Motivation}

We begin with the following, ideal problem: 
\begin{definition}[Duplicate detection problem, DDP] Given a stream $E_n = \{e_1, e_2, \dotsc, e_n\}$ and a \enquote{new} item $e^\star$, find whether $e^\star \in E_n$.\footnote{
	We do not consider here \emph{fuzzy} detection approaches\cite{10.5555/1287369.1287420,1410199,10.1109/ICDE.2011.5767865,10.1109/ICDE.2012.20,Monge97anefficient} that study situations where $e^\star$ belongs to $E_n$ when there exists a \enquote{similar enough} element in the stream.
}
	At every time increment, the new item is added to the stream, i.e., $E_{n+1} = E_n \cup \{e^\star\}$. 
\end{definition}
Instances of this problem abound in computer science, with applications in file system indexation, database queries, network load balancing, network management \cite{10.5555/647912.740658}, in credit card fraud detection \cite{DBLP:journals/corr/abs-1709-08920}, phone calls data mining \cite{10.1145/347090.347094}, etc. A discussion about algorithms on large data streams can be found in \cite{10.1145/776985.776986}. 

In practice, additional constraints exist that we can capture with the following definition:
\begin{definition}[DDP with bounded memory]
	At every time step $n$, given $e^{\star}$ and a current state (dependent on history) of at most $M$ bits, solve the DDP for $E_n$ and $e^{\star}$.
\end{definition}
%Even when the stream is bounded, i.e. $\lim_{n\to\infty}|E_n| < \infty$, it may be impractical to perform a complete lookup and approximate solutions are desirable. 
One can prove \cite[Theorem 2.1]{10.1145/3297280.3297335} that perfect detection of duplicates in a stream with $U$ possible values requires $U$ bits of memory: even for modest situations (e.g. 64-bit nonces, corresponding to $2^{64}$ possible values) this is quickly impractical. As a result, we need a further relaxation of the DDP that allows for errors: 
\begin{definition}[Approximate DDP with bounded memory]
	Solve the DDP using at most $M$ bits of memory, with at most $\epsilon < 1/2$ mispredictions.\footnote{Given a classifier with $\epsilon > 1/2$, we obtain a classifier with $\epsilon < 1/2$ by reversing its output.} 
\end{definition}
Approximate duplicate detection has many real-life use cases, and can sometimes play a critical role, for instance in cryptographic schemes where all security and secrecy fall apart as soon as a random nonce is used twice, such as the ElGamal \cite{10.1007/3-540-39568-7_2} or ECDSA signatures. Other uses include improvements over caches \cite{4484874}, duplicate clicks \cite{10.1145/1060745.1060753}, and others.

Finally, it is clear that the input distribution plays a central role regarding how efficiently the DDP can be solved. For instance, some deterministic streams may be expressed very compactly (such as the output of a PRNG with known seed) making the DDP relatively easy. Information-theoretically, if the source has $U$ bits of entropy then the situation is equivalent to having an $U$-bit, uniformly distributed input.\footnote{Another motivation for this is pragmatic: in many applications, duplicate detection is performed on \emph{hashes} of actual data rather than on the data itself, and we may assume that hashes are uniformly distributed.} 

\todo[continuer].

\subsection{Contributions}

\subsection{Related Work}\label{sub:related}
\todo[continuer]
In \cite{10.1145/1060745.1060753}, the authors explore the behaviour of a structure very close to the counting Bloom filter in some specific sliding window context, that they call landmark- and jumping sliding windows. However, they do not deal with the problematic of a sliding window where at each new element added to the sliding window, another element is removed.
Closer to our work, the idea of using subfilters, tossing old ones has already been explored in the A2 filter \cite{Yoo10}. However, the author only considered a small portion of the problem, as they only discussed about using two subfilters, and only Bloom Filters acting as subfilters.

In this work, we provide a full abstraction of the mechanism, allowing us to use many other DDF than Bloom Filters. We also provide a generalised analysis of the error rates.
\subsection{Organisation}


\section{Duplicate Detection and Filter Saturation}
\subsection{Notations and Initial Problem Statement}
Let us consider an alphabet $\Gamma$. A stream $E$ over $\Gamma$ is a (possibly infinite) set $E = \{e_1, e_2, \dotsc\}$, with each $e_i \in \Gamma$.

\begin{definition}[Duplicate and unseen element]
Let $E = \{e_1, e_2, \dotsc\}$ be a stream over $\Gamma$. Some element $e_j \in E$ is called a \emph{duplicate} if and only if there exists $i < j$ such that $e_i = e_j$. In that case we write $e_j \dup E$. Otherwise, $e_j$ is said to be \emph{unseen} and we write $e_j \notdup E$.
\end{definition}

\begin{definition}[Duplicate detection filter]
	A \emph{duplicate detection filter} (DDF) is a collection 
	$\mathcal F = (\mathcal S, \mathsf{Insert}, \mathsf{Detect})$ of a state\footnote{Here, $\mathcal M$ denotes all the memory states that can be reached by the filter.} $\mathcal S \in \mathcal M$ and two algorithms,
	\begin{itemize}
		\item $\mathsf{Detect}: \mathcal M \times \Gamma \to \{\DUPLICATE, \UNSEEN\}$;
		\item $\mathsf{Insert}: \mathcal M \times \Gamma \to \mathcal M$.
	\end{itemize}
	Informally, the $\mathsf{Detect}$ algorithm tentatively labels an element $e$ as duplicate or not, given the current state $\mathcal S$; whereas $\mathsf{Insert}$ takes as input the current state, an element $e$ to insert in the filter, and returns an updated memory state $\mathcal S'$. 
\end{definition}

We will usually consider the situations where the available memory is too small for perfect detection, i.e., $|\mathcal S| \ll |\Gamma|$.

\begin{definition}[False positive, false negative]
	Let $e \dup E$ (resp. $e \notdup E$) and $\mathcal F$ be a filter. We say that $e$ is a \emph{false positive} (resp. \emph{false negative}) when $\mathcal F.\mathsf{Detect}(e) = \DUPLICATE$ (resp. $\UNSEEN$).
\end{definition}

\begin{definition}[False positive rate, false negative rate]
	Let $E$ be a stream, $\mathcal F$ be a filter. The \emph{false positive rate} (resp. \emph{false negative rate}) of $\mathcal F$ over the first $i < n$ elements, denoted $\FPR_i$ (resp. $\FNR_i$), is defined as the ratio of false positives divided over the number of unseen elements in $\{e_1, \dotsc, e_i\}\subset E$ (resp. the ratio of false negative divided by the number of duplicate elements in that subset).
\end{definition}

However, it is more practical to work with another value, namely, the probability that after $m$ insertions, the element $e^\star$ is a false positive (resp. false negative). 

\begin{definition}[Probability of false positive, of false negative]
	\label{def:pfp-pfn}
	Let $E = \{e_1, \dotsc\}$ be a stream, $\mathcal F$ be a filter. The \emph{probability of false positive} (resp. \emph{probability of false negative}) of $\mathcal F$ after $i$ elements, denoted $\FP_i$ (resp. $\FN_i$), is defined as the probability that $e_{i+1}$ is a false positive (resp. false negative).
\end{definition}

One can observe that $\FPR_n = \frac{1}{n}\sum_{i=1}^n \FP_i$, and similarly for $\FNR_n$.

\subsection{Filter Saturation and Sliding Windows}
Unfortunately, efficient duplicate detection over an infinite stream has been identified as an unsolvable problem \cite{10.1145/3297280.3297335}. As a matter of fact, as the number of bits of information of the stream grow to infinity, the filter can only remember an ever diminishing proportion of those bits, eventually having no advantage compared to a filter answering randomly.

In other terms, any DDF is asymptotically as good as a random guess:
	\begin{theorem}[\cite{10.1145/3297280.3297335}]\label{thm:sat_u}
	Let $\FNR_\infty$ and $\FPR_\infty$ be the asymptotic false negative and false positive rates of a filter of $M$ bits of memory (i.e. the FNR and FPR on a stream of size going to infinity).
	
	If $|\Gamma| \gg M$, then $\FNR_\infty + \FPR_\infty = 1$, which characterises random filters (i.e. filters answering randomly to any query).
\end{theorem}

One way out of this situation is to lower our expectations and work on a sliding window:

\begin{definition}[Sliding window DDP]
	Given a stream $E_n$ of fixed size $w$, with $E_n = \{e_{n-w+1}, \dotsc, e_n\}$ and an item $e^\star$, find whether $e^\star \in E_n$.
\end{definition} 

We therefore introduce new definitions suited to that setting:

\begin{definition}[Duplicate element, unseen element over a sliding window]
	Let $E = \{e_1, e_2, \dotsc\}$ be a stream over $\Gamma$, let $w\in \mathbb N$, an element $e_j \in E$ is called a \emph{duplicate} in $E$ over the sliding window of size $w$ if and only if there exists $j$ such that $j - w \leq i < j$  and $e_i = e_j$. In this case we write $e_j \wdup E$. Otherwise, $e_j$ is said to be \emph{unseen} and we write $e_j \notwdup E$.
\end{definition}

\begin{definition}[False positive, false negative over a sliding window]
	Let $w \in \mathbb N$, $e \notwdup E$ (resp. $e \wdup E$) and let $\mathcal F$ be a DDF. We say that $e$ is a \emph{false positive} (resp. \emph{false negative}) over $w$ if $\mathsf{Detect}(e) = \DUPLICATE$ (resp. $\UNSEEN$).
\end{definition}

\begin{definition}[False positive rate, false negative rate over a sliding window]
	Let $E$ be a stream, $\mathcal F$ be a DDF, $w \in \mathbb N$. The \emph{false positive} (resp. \emph{false negative}) rate of $\mathcal F$ over the first $i < n$ elements and the sliding window $w$, noted $\FPR^w_i$ (resp. $\FNR^w_i$), is defined as the number of false positives over $w$ divided by the number of unseen elements in $E$ over $w$ (resp. the number of false negative over $w$ divided by the number of duplicate elements over $w$).
\end{definition}


\subsection{Bounds on the Sliding Window DDP}\label{sub:opt_solutions}

Our goal is to minimise both the false positive and false negative rates of a DDF, given a fixed and limited amount of memory. Before going into details, we first explore the limits of the problem, i.e., when perfect or very performant solutions exist.

\begin{theorem}
	For $M \geq w (\log_2(w) + 1) \log_2(|\Gamma|)$, the sliding window DDP can be solved exactly (with no errors) in constant time.
\end{theorem}

\begin{proof}
	We explicitly construct a DDF that performs the detection. Storing all $w$ elements in the sliding window takes $w\log_2(|\Gamma|)$ memory, using a FIFO queue $Q$; however 
	lookup has a worst-time complexity of $O(w)$. 
	
	We therefore rely on an
	ancillary data structure for the sake of quickly answering lookup questions.
	Namely we use a dictionary $D$ whose keys are elements from $\Gamma$ and values are counters.
	
	Whe an element $e$ is inserted in the DDF, $e$ is stored and $D[e]$ is incremented (if the key $e$ did not exist in $D$, it is created first, and
	$D[e]$ is set to $1$). In order to keep the number of stored elements to $w$, 
	we discard the oldest element $e_\text{last}$ in $Q$. As we do so, we also decrement $D[e_\text{last}]$, and if $D[e_{last}] = 0$ the key is deleted from $D$. The whole insertion procedure is therefore performed in constant time.
	
	Lookup of an element $e^\star$ is simply done by looking whether the key $D[e^\star]$ exists, which is done in constant time.
	
	The size of the queue is $w\log_2 |\Gamma|$, the size of the dictionary is $w \log_2 |\Gamma| \log_2 w$ (as the dictionary cannot have more than $w$ keys as the same time, and a counter cannot go over $w$, thus being less than $\log_2 w$ bits long). Thus at most $w (\log_2(w) + 1) \log_2(|\Gamma|)$ bits are necessary for this DDF to work.

	Finally this filter does not make any mistake, as the dictionary $D$ keeps an exact tract of how many times each element was present in the sliding window.
\end{proof}
The dependence on $\log_2 |\Gamma|$ can be droped, at the cost of allowing 
errors:
\begin{theorem}
	Let $w \in \mathbb N$. Let $M \simeq 2w\log_2w$, then the sliding window DDP can be solved with almost no error using $M$ bits of memory.
	
	More precisely, it is possible to create a filter of $M$ bits with an FNR of $0$, and an FPR of $1 - (1-\frac1{w^2})^w \sim \frac 1w$, and a time complexity of $O(w)$.
	
	Using $M \simeq 5w\log_2 w$ bits of memory, a constant-time filter with the same error rate can be constructed.
\end{theorem}

\begin{proof}
	Here again we explicitely construct the filters that achieve the theorem's bounds.

	Let $h$ be a hash function with codomain $\{0,1\}^{2 \log_2 w}$.
The birthday theorem, as summed up in \cite{10.1007/3-540-45708-9_19}, states that for a hash function $h$ over $a$ bits, one must on average collect $2^{a/2}$ input-output pairs before obtaining a collision. Therefore $2^{(2 \log_2 w) / 2} = w$ hash values $h(e_i)$ can be computed before having a $50\%$ probability of a collision (here, a collision is when two distinct elements of the stream $e_i, e_j$ with $i \neq j, e_i \neq e_j$ have the same hash, i.e. $h(e_i) = h(e_j)$). The 50\% threshold we impose on $h$ is arbitrary but nonetheless practical.

Let $\mathcal F$ be the following DDF: the filter's state consists in a queue of $w$ hashes, and for each new element $e$, $\mathsf{Detect}(e)$ returns \DUPLICATE if $h(e)$ is present in the queue, \UNSEEN otherwise. $\mathsf{Insert}(e)$ appends $h(e)$ to the queue before popping the queue. 

There is no false negative, and a false positive only happens if the new element to be inserted collides with at least one other element, which happens with probability $1 - (1 - \frac 1{2^{2\log_2w}})^{w} = 1 - (1-\frac1{w^2})^w$, hence an FNR of $0$ and a FPR of $1 - (1-\frac1{w^2})^w$.
The queue stores $w$ hashes, and as such requires $w \cdot 2\log_2 w$ bits of memory.

Note that this solution has a time complexity of $O(w)$. Using an additional dictionary, as in the previous proof, we get a filter with an error rate of about $\frac 1w$ and constant time for insertion and lookup, using $5w\log_2 w$ bits of memory.
\end{proof}

When $\log_2|\Gamma| > 2 \log_2 w$ (which is the case of most practical use cases) this DDF outperforms the naïve strategy\footnote{The naïve strategy consisting of storing the $w$ elements of the sliding window, requiring $w \log_2|\Gamma|$ bits of memory.}, both in terms of time and memory.


Hence, very good solutions for situations where $M \approx 5w\log_2 w$ already exist, but efficient solutions and lower bounds on the error rates for smaller amounts of memory are still to be found.


\subsection{DDP vs Sliding Window DDP}
The sliding window variant of DDP may seem to be a slight weakening of the original problem. Remarkably, it turns out that DDFs which perform well for the DDP do rather poorly for the sliding window DDP. A literature review collects the following DDF constructions: A2 filters \cite{Yoo10}, Stable Bloom Filters (SBF) \cite{Den06}, Quotient Hash Tables (QHT) \cite{10.1145/3297280.3297335}, Streaming Quotient Filters (SQF) \cite{Dut13}, Block-decaying Bloom Filters (b\_DBF) \cite{She08}, and a slight variation of Cuckoo Filters \cite{Fan14} suggested by \cite{10.1145/3297280.3297335}. The structure proposed in \cite{10.1145/1060745.1060753} is not suited to either sliding window or no sliding window DDP, as they work on what they call `landmark` sliding window, which consists of a zero-resetting of the memory at some user-defined epochs.

Of these, only the A2 and b\_DBF were designed in a way immediately applicable to the sliding window DDP. The others do not account for a finite-length sliding window, and therefore cannot be adjusted as a function of $w$. Another remarkable (but experimental) observation is that these DDFs' false positive rate does not decrease to 0 when the sliding window becomes small.

\todo[Expliquer ?]


\section{Queuing filters}
In this section, we describe the queuing construction, which produces a sliding window DDF from any DDF. We first give the description of the setup, before studying the theoretical error rates. 
A scheme describing our structure is detailed in Figure~\ref{fig:scheme}.

\subsection{The queueing construction}

\paragraph{Principle of operation.}
Let $\mathcal F$ be a DDF. Rather than allocating the whole memory to $\mathcal F$, we will create $L$ copies of $\mathcal F$, each using a fraction of the available memory. Each of these \emph{subfilters} as a limited timespan, and is allowed up to $c$ insertions. The subfilters are organised in a queue.

When inserting a new element in the queueing filter, it is inserted in the topmost subfilter of the queue. After $c$ insertions, a new empty filter is added to the queue, and the oldest subfilter is popped and erased.

As such, we can consider that each subfilter operates on a sub-sliding window of size $c$, which makes the overall construction a DDF operating over a sliding window of size $w = cL$.

\begin{figure}
	\input{fig/queuef.tex}
	\caption{Architecture of the queuing filter. The filter is composed of $L$ subfilters $\mathcal F$, each containing up to $c$ elements. Once the newest subfilter has inserted $c$ elements in its structure, the oldest one is expired. As such, it is dropped and a new one is created and put under population at the beginning of the queue. In this example, the sub-sliding window of $\mathcal F_1$ is $\{e_{m-2}, e_{m-3}, e_{m-4}\}$.} \label{fig:scheme}
\end{figure}

\paragraph{Insertion and lookup.}
The filter returns \DUPLICATE if and only if at least one subfilter does. Insertion is a simple insertion in the topmost subfilter.

\paragraph{Queue update.}
After $c$ insertions, the last filter of the queue is dropped, and a new (empty) filter is appended in front of the queue.

\paragraph{Pseudocode.}
We give a brief pseudo-code of the queuing filter's functions \textsf{Lookup} and \textsf{Insert}, as well as a \textsf{Setup} function for initialisation, in Algorithm~\ref{alg:queueing}. We introduced for simplicity a constructor $\mathcal F.\textsf{Setup}$ that takes as input an integer $M$ and outputs an initialized empty filter $\mathcal F$ of size at most $M$. The \texttt{subfilters} is a FIFO that has a \texttt{pop} and \texttt{push\_first} operations, which respectevely removes the last element in the queue or inserts a new item in first  position.   

\begin{algorithm}[]
	\caption{Queueing Filter Setup, Lookup and Insert}\label{alg:queueing}
	
	\begin{algorithmic}[1]
	\Function{Setup}{$\mathcal F, M, L, c$}
		\Comment $M$ is the available memory, $\mathcal F$ the subfilter structure, $L$ the number of subfilters and $c$ the number of insertions per subfilter
		\State subfilters $\gets \emptyset$
		\State counter $\gets 0$
		\State $m \gets \lfloor M/L \rfloor$
		\For{$i$ from $0$ to $L-1$}
			\State subfilters.push\_first$(\mathcal F.\mathsf{Setup}(m))$
		\EndFor
		\State \textbf{store} (subfilters, $L$, $m$, counter)
		\State\Return filter
	\EndFunction 
	\item[]
	\end{algorithmic}

	\begin{algorithmic}[1]
	\Function{Lookup}{$e$}
		\For{$i$ from $0$ to $L-1$}
			\If{subfilters[i]$.\mathsf{Lookup(e)}$}
				\State \Return $\DUPLICATE$
			\EndIf
		\EndFor
		\State \Return \UNSEEN
	\EndFunction
	\item[]
	\end{algorithmic}

	\begin{algorithmic}[1]
	\Function{Insert}{$e$}
		\State subfilters[0]$.\mathsf{Insert}(e)$
		\State counter $\gets$ counter + 1 
		\If{counter == $c$}
			\State subfilters.pop()
			\State subfilters.push\_first($\mathcal F.\textsf{Setup}(m)$)
		\EndIf
	\EndFunction
	\end{algorithmic}
\end{algorithm}

\subsection{Error rate analysis}
The queueing filter's properties can be derived from the subfilters'. False positive and false negative rates are of particular interest. In this section we consider a queuing filter $\mathcal Q$ with $L$ subfilters of type $\mathcal F$ and 
capacity $c$ (which means that the last subfilter is dropped after $c$ insertions).

\paragraph{Remark.}
	\label{subsub:number_elements}
	By definition, after $c$ insertions the last subfilter is dropped.
Information-theoretically, this means that all the information related to the elements inserted in that subfilter has been lost, and there are $c$ such elements by design. 
Therefore, in the steady-state regime, the queuing filter holds information about at least $c(L-1)$  elements (immediately after deleting the last subfilter) and at most $cL - 1$ elements (immediately before this deletion). 

As a result, if $w < cL$, the queuing filter can hold information about \emph{more than $w$ elements}.

\subsubsection{Probability of False Positive}

\begin{theorem}
	\label{thm:queue-pfp}
	 Let $\FP_{\mathcal Q, m}^w$ be the probability of false positive (def. \ref{def:pfp-pfn}) of $\mathcal Q$ after $m > w$ insertions, over a sliding window of size $w = cL$, we have:
	\begin{equation}
		\label{eq:queue-pfp}
		\FP_{\mathcal Q, m}^w = 1 - \left(1 - \FP_{\mathcal F,c}\right)^{L-1} \left(1 - \FP_{\mathcal F, m \bmod c }\right)
	\end{equation}
	where $\bmod$ is the modulo operator and 
	 $\FP_{\mathcal F,m}$ be the probability of false positive a subfilter $\mathcal F$ after $m$ insertions.
\end{theorem}

\begin{proof}
	Let $E = \{e_1, \dotsc, e_m, \dotsc\}$ be a stream and $e^\star \notwdup E$. 
	
	Therefore, $e^\star$ is a false positive if and only if at least one subquery $\mathcal F_i.\mathsf{Lookup(e^\star)}$ returns \DUPLICATE. Conversely, $e^\star$ is \emph{not} a false positive when all subqueries $\mathcal F_i.\mathsf{Lookup(e^\star)}$ return \UNSEEN, i.e., when $e^\star$ is not a false positive for each subfilter.
	
	Each subfilter has undergone $c$ insertions, safe for the first subfilter which has only undergone $m \bmod c$, we immediately get Eq.~(\ref{eq:queue-pfp}).
\end{proof}

\paragraph{Remark.} In the case $w < cL$, as mentioned previously, there is a non-zero probability that $e^\star$ is in the last subfilter's memory, despite not belonging to the sliding window. 
	
Assuming a uniformly random input stream, and writing $\delta = cL - w$, the probability that $e^\star$ has occurred in $\{e_{m-cL}, \dotsc e_{m-w+1}\}$ is $1-\left(1-\frac1{|\Gamma|}\right)^\delta$. For large $\Gamma$ (as is expected to be the case in most applications), this probability is about $\frac{\delta}{|\Gamma|} \ll 1$. Hence, we can neglect the probability that $e^\star$ is present in the filter, and we consider the result of Theorem~\ref{thm:queue-pfp} to be a very good approximation even when $w < cL$.

\subsubsection{Probability of False Negative}\label{sub:fnr}
\begin{theorem}
	\label{thm:queue-pfn}
	Let $\FN_{\mathcal Q, m}^w$ be the probability of false negative of $\mathcal Q$ after $m > w$ insertions on a sliding window of size $w = cL$, we have
	\begin{equation}
	\FN_{\mathcal Q, m}^w 
	=  u_c^{L-1} u_{m \bmod c}
	\end{equation}
	where $\bmod$ is the modulo operator, and we have introduced the short-hand notation $u_\eta= p_{\eta}\FN_{\mathcal F,\eta} + \left(1-p_{\eta}\right)\left(1-\FP_{\mathcal F,\eta}\right)$ where 
	$\FN_{\mathcal F,\eta}$ (resp. $\FP_{\mathcal F, \eta}$) is the probability of false negative (resp. false positive) of the subfilter $\mathcal F$ after $\eta$ insertions, and
	\begin{equation*}
		p_\eta = \frac{1-\left(1-\frac 1{|\Gamma|}\right)^\eta}{1-\left(1-\frac 1{|\Gamma|}\right)^w} \approx \frac{\eta}{w}.
	\end{equation*}
\end{theorem}

\begin{proof}
	Let $E = \{e_1, \dotsc, e_m, \dotsc\}$ be a stream, let $w$ be a sliding window and let $e^\star \wdup E$. 
	
	Then $e^\star$ is a false negative if and only if all subfilters $\mathcal F_i$ answer $\mathcal F_i.\mathsf{Detect}(e^\star) = \UNSEEN$. There can be two cases:
	\begin{itemize}
		\item $e^\star$ is present in $\mathcal F_i$'s sub-sliding window;
		\item $e^\star$ is not present in $\mathcal F_i$'s sub-sliding window.
	\end{itemize}
In the first case, $\mathcal F_i.\mathsf{Detect}(e^\star)$ returns \UNSEEN if and only if $e^\star$ is a false negative for $\mathcal F_i$. This happens with probability $\FN_{\mathcal F,c}$ by definition, except for $\mathcal F_0$, for which the probability is $\FN_{\mathcal F, m\bmod c}$.

In the second case, $\mathcal F_i.\mathsf{Detect}(e^\star)$ returns \UNSEEN if and only if $e^\star$ is not a false positive for $\mathcal F_i$, which happens with probability $1-\FP_{\mathcal F,c}$, execpt for $\mathcal F_0$, for which the probability is $1-\FP_{\mathcal F, m\bmod c}$.

Finally, each event is weighted by the probability $p_c$ that $e^\star$ is in $\mathcal F_i$'s sub-sliding window:
\begin{align*}
	p_c 
	& = \Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window | } e^\star \wdup E]\\
	& = \frac{\Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window } \cap e^\star \wdup E]}{\Pr[e^\star \wdup E]}\\
	& = \frac{\Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window}]}{\Pr[e^\star \wdup E]}\\
	& = \frac{1 - \Pr[\text{$e^\star$ is not in }\mathcal F_i \text{ sub-sliding window }]}{1 - \Pr[e^\star \notwdup E]}\\
	& = \frac{1 - \left(1 - \frac 1{|\Gamma|}\right)^c}{1 - \left(1-\frac{1}{|\Gamma|}\right)^w}
\end{align*}
This concludes the proof.
\end{proof}
\paragraph{Remark.} As previously, the effect of $w < cL$ is negiglible for all practical purposes and Theorem~\ref{thm:queue-pfn} is considered a good approximation in that regime. 

\subsection{FNR and FPR}

As stated before, the probabilty of false positive and false negative are not the usual measurement made when benchmarking filters. Rather, one prefers to use false positive (and negative) \emph{rates}, defined as the average of the false positive (and negative) probabilities encountered so far.

From the above expressions we can derive relatively compact explicit formulas for the queuing filter's FPR and FNR. For the sake of simplicity, we only give the FPR and FNR of a queueing filter $\mathcal Q$ after $m = cn$ insertions, where $n$ is a positive integer.
\begin{theorem}
	Let $\FPR_{\mathcal Q, m}^w$ be the false positive rate of $\mathcal Q$ after $m=cn > w$ insertions on a sliding window of size $w \approx cL$, we have
	$$\FPR_{\mathcal Q, cn}^w = 1 -  (1 - \FP_{\mathcal F, c})^{L-1}\sum_{\ell = 0}^{c-1} (1-\FP_{\mathcal F, \ell})$$
\end{theorem}
\begin{proof}


\begin{align*}
	\FPR_{\mathcal Q, cn}^w
	& = \frac{1}{cn} \sum_{k=1}^{cn} \FP_{\mathcal Q, k}^w 
	 = \frac{1}{cn} \sum_{k=1}^{n} \sum_{\ell = 0}^{c-1} \FP_{\mathcal Q, k + \ell}^w 
	 = \frac1c \sum_{\ell = 0}^{c-1} \FP_{\mathcal Q, \ell}^w \\
	& = \frac1c \sum_{\ell = 0}^{c-1}  1 - (1 - \FP_{\mathcal F, c})^{L-1}(1 - \FP_{\mathcal F}, \ell) \\
	& = 1 -  (1 - \FP_{\mathcal F, c})^{L-1}\sum_{\ell = 0}^{c-1} (1-\FP_{\mathcal F, \ell})
\end{align*}

\end{proof}

\begin{theorem}
	Let $\FNR_{\mathcal Q, m}^w$ be the false negative rate of $\mathcal Q$ after $m=cn > w$ insertions on a sliding window of size $w \approx cL$, we have
	
	$$\FNR_{\mathcal Q, cn}^w = \frac{u_c^{L-1}}{c}  \sum_{\ell = 0}^{c-1}u_\ell$$
	
	with the $u_\eta= p_{\eta}\FN_{\mathcal F,\eta} + \left(1-p_{\eta}\right)\left(1-\FP_{\mathcal F,\eta}\right)$.
\end{theorem}

\begin{proof}
\begin{align*}
	\FNR_{\mathcal Q, cn}^w 
	& = \frac{1}{cn} \sum_{k=1}^{cn} \FN_{\mathcal Q, k}^w 
	 = \frac{1}{cn} \sum_{k=1}^{n} \sum_{\ell = 0}^{c-1} \FN_{\mathcal Q, k + \ell}^w 
	= \frac1c \sum_{\ell = 0}^{c-1} \FN_{\mathcal Q, \ell}^w \\
	& = \frac1c \sum_{\ell = 0}^{c-1} u_c^{L-1} u_{\ell} 
	 = \frac{u_c^{L-1}}{c}  \sum_{\ell = 0}^{c-1}u_\ell
\end{align*}

\end{proof}


\section{Optimising queuing filters}
\todo[À vérifier je suis pas convaincu encore]
\begin{theorem}
	In first approximation, a low value of $L$ for constant value $cL$ leads to a higher false negative probability for the queueing filter.
\end{theorem}

\begin{proof}
As a matter of fact, on average the queueing filter consists of $L-1$ subfilters with $c$ insertions, and one subfilter with $\frac c2$ insertions. As such, we can say that the queueing filter is a representation of the last $(c-\frac12) L$ last elements. 

Let us consider a duplicate $e^\star \wdup E$. If, in the sliding window, all duplicates only happened at least $(c-\frac12)L$ epochs ago (which happens with probability $\left(1-\frac1{|\Gamma|}\right)^{(c-\frac12)L}$), we know that no duplicates happened in any subfilter's sliding window. By using the same reasoning as Theorem~\ref{thm:queue-pfn}'s proof, we get a probability of false negative of around $\left[0 + \left(1-\frac1{|\Gamma|}\right)^{(c-\frac12)L}(1-\FP_{\mathcal F,c})\right]^L$. Given that $cL \approx w$ which is a constant, we get that the probability of false positive mostly varies, in this case, with $\left(1-\FP_{\mathcal F,c}\right)^L$. 

Hence, the lower $L$ is, the higher the probability of false negative will be.

The approximations made in this proof are backed by experimental results.
\end{proof}


\section{Application to existing DDFs}
Our queuing construction relies on a choice of subfilters. A first observation 
is that we may assume that all subfilters can be instances of a single 
DDF design (rather than a combination of different designs).

Indeed, a simple symmetry argument shows that a heterogenous selection of subfilters is always worse than a homogeneous one: the crux is that all subfilters play the same role in turn. Therefore we lose nothing by replacing atomically one subfilter by a more efficient one. Applying this to each subfilter we end up with a homogenous selection.

It remains to decide which subfilter construction to choose. 

\subsection{Subfilter selection}
When comparing different DDFs, one interesting metric in our context is how fast
such filters saturate: indeed, in the original DDP setting, all filters eventually saturate \cite{10.1145/3297280.3297335}. 

\begin{theorem}
	Let $E$ be a stream of $n$ elements uniformly selected from an alphabet of size $|\Gamma|$. For any DDF using $M$ bits of memory, the error probability $ER_n = \FP_n + \FN_n$ satisfies
	\begin{equation}
		ER_n \geq 1 - \frac{1 - \left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n} 
	\end{equation}
	for any $n > M$.
\end{theorem}
	In particular, the asymptotic error rate $ER_\infty$ satisfies $ER_\infty \geq \left(1 - \frac{1}{|\Gamma|}\right)^{M} \approx 1 - M/|\Gamma|$.

\begin{proof}
	By definition, a perfect filter has the lowest possible error rate.
	With $M$ bits of memory, a perfect filter can store at most $M$ elements in memory \cite[Theorem 2.1]{10.1145/3297280.3297335}. Up to reordering the stream, without loss of generality because it is random, we may assumethat the filter stores the $M$ last elements of the stream: any other strategy cannot yield a strictly lower error rate.
	
	If an element is already stored in the filter, then the optimal filter will necessarily answer \DUPLICATE. On the other hand, if the element is not in memory, a perfect filter can choose to answer randomly. Let $p$ be the probability that a filter answers \DUPLICATE when an element is not in memory. An optimal filter will lower the error rate of any filter using the same strategy with a different probability.
	
	An unseen element, by definition, will be unseen in the $M$ last elements of the stream, and hence will not be in the filter's memory, so the filter will return \UNSEEN with probability $1-p$. For this reason, this filter has an FP probability of $p$.
	
	On the other hand, a duplicate element $e^\star \dup E$ is classified as \UNSEEN if and only if it was not seen in the last $M$ elements of the stream, and the filter answers \UNSEEN. Let $D$ be the event \enquote{\emph{There is at least one duplicate in the stream}} and $C$ be the event \enquote{\emph{There is a duplicate of $e^\star$ in the $M$ previous elements of the stream}}. Then $e^\star$ triggers a false negative with probability
	\begin{align*}
	\FN_n &= (1 - \Pr[C | D])(1-p) =  \left(1 - \frac{\Pr[C \cap D]}{\Pr[D]}\right)(1-p)\\
	& = \left(1 - \frac{\Pr[C]}{\Pr[D]}\right)(1-p) =  \left(1 - \frac{1 - \Pr[\bar C]}{1 - \Pr[\bar D]}\right)(1-p) \\
	\FN_n &=  \left(1 - \frac{1 - \left(1 - \frac 1{|\Gamma|}\right)^M}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}\right)(1-p)
	\end{align*}
	Hence, the error probability of the filter is
	\begin{align*}
		ER_n 
		& = \FN_n + p \\
		& =  \left(1 - \frac{1 - \left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}\right)(1-p) + p \\
		& = 1 - \frac{1-\left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}(1-p),
	\end{align*}
	which is minimized when $p = 0$. 
	\end{proof}

Note, as highlighted in the proof, that this bound is \emph{not tight}: better bounds may exist, the study of which we leave as an open question for future work. 

The results of an experimental comparison of different DDFs (details about the benchmark are given in Section~\ref{sub:saturation}) are summarizes in Figure~\ref{fig:graph_n}. It appears that the two most efficient filters (in terms of saturation rate) are the QHT \cite{10.1145/3297280.3297335} and the A2 \cite{Yoo10}.

As noted in Section~\ref{sub:related}, A2 filters are a primitive form of a queuing filter (with $L=2$ and Bloom filters as subfilters) and thus were not included in this benchmark, as we focused on adapting filters designed for DDP to the sliding window DDP. This raises the question of whether recursive constructions (queuing filters of queuing filters) have particularly interesting properties, which we leave open for future work. 

In the experiments we use QHT for the benchmarking of the queueing construction.

\subsection{Parameter Tuning}
\todo[FAUX, peut etre log ? However, practical tests show (see Section~\ref{sub:L_bench}) that setting $L = \sqrt w$ leads to optimal results.]

The parameters of a queueing filter are: the parameters of the subfilter, the number $L$ of subfilters and the number $c$ of insertions per subfilter. 

However, the goal here is not to modify the parameters of the underlying subfilter, but rather to tune the queueing filter so the subfilter can perform as well as possible. Hence, the queueing filter's parameters are only $c$ and $L$.

Furthermore, we have the relation $cL \approx w$, so there is only one free parameter. 

As such, an optimal parameter tuning would be to set $L$ so that the error rate (defined as the sum of the FPR and FNR) would be minimal. Nonetheless, given the complicated nature of the FPR and FNR formulae, deriving an optimal $L$ is out of reach.

We have observed in Section~\ref{sub:fnr} that the FN is positively correlated with
$\left(1-\FP_{\mathcal F,c}\right)^L$, however the FP is negatively correlated with the same value. As such, decreasing the probability of false positive leads to an increase in the probability of false negatives. 


%Hence, the (heuristic) optimal solution is to set $L = \sqrt w$.


\section{Experiments and Benchmarks}
This section provides details and additional information on the benchmarking experiments run to validate the above analysis. All code is accessible online and will be disclosed after peer review.

Our benchmarks cover two aspects: 
\begin{enumerate}
\item A measure of DDFs' saturation rate, in the original DDP setting (i.e. without a sliding window), which is used to choose a subfilter design;
\item A measure of the effect of queuing on error rates, in the sliding window DDP setting. Namely, we compate QHT and a queuing QHT.
\end{enumerate}
An important note is that when the sliding window is small compared to the filter's memory size, optimal or nearly optimal solutions can be implemented (see Section~\ref{sub:opt_solutions}). This is practical for smaller values because we can measure exactly a filter's response compared to the \enquote{ground truth}. However, this is no longer possible for very large parameters as it would require too much memory for keeping track of the duplicates.

\subsection{Saturation Resistance of DDFs}\label{sub:saturation}
We evaluate the saturation rate for several DDFs, in the original DDP setting (without sliding window). Parameters are chosen to yield equivalent memory footprints and agree with \cite{10.1145/3297280.3297335}, namely:
%
\begin{itemize}
	\item QHT \cite{10.1145/3297280.3297335}, 1 bucket per row, 3 bits per fingerprint.
	\item SQF \cite{Dut13}, 1 bucket per row, $r = 2$ and $r' = 1$
	\item Cuckoo Filter \cite{Fan14}, cells containing 1 element of 3 bits each
	\item Stable Bloom Filter (SBF) \cite{Den06}, 2 bits per cell, 2 hash functions, targeted FPR of 0.02
	\item $A2$ Filter \cite{Yoo10}, targeted FPR of $0.1$ on the sliding window.
%	\item Block-Decaying Bloom Filter (b\_DBF) \cite{She08}, sliding window of 6000 elements.
\end{itemize}
%
These filters are run against a stream of uniformly sampled elements from an alphabet of $2^{26}$ elements. This results in around 8\% duplicates (up to statistical fluctuations) amongst the 150 000 000 elements in the longest stream used. As a comparison, we also run the same benchmark against a stream obtained from crawling data, a real-world source which is highly non-uniform --- note that in this case, our lower bound on the error rate does not apply, owing to the stream being compressible. Results are plotted in Figure~\ref{fig:graph_n}.

\begin{figure*}[!ht]
	\begin{tikzpicture}
	\begin{groupplot}[
	group style = {
		group size = 2 by 1,
		horizontal sep = 35pt,
	},
	axis lines = left,
	ylabel near ticks,
	width = 0.4\textwidth,
	height = 0.27\textwidth,
	]
	\input{er_n_artificial}
	\input{er_n_real}
	%\input{10e6_sum_zipf}
	\end{groupplot}
    \path (top)--(bot) coordinate[midway] (group center);
%	\node[above,rotate=90] at (group center -| current bounding box.west) {Error rate (\%)};
	\node[right=1em,inner sep=0pt] at(group center -| current bounding box.east) {\pgfplotslegendfromname{legend_n}};
	\end{tikzpicture}

%	\input{er_n_artificial}
	\caption{Error rate (times 100) of DDFs of 1Mb, over a real and artificial stream, depending on the size of the stream, without any sliding window. Hatched area represents over-optimal (impossible) values.}\label{fig:graph_n}
\end{figure*}

We observe that the best filters are, in order, QHT, SQF, Cuckoo and SBF. We also observe that QHT and SQF have error rates relatively close to the lower bound, hence suggesting that these filters are close to optimality.

\subsection{Benchmarking Various Queuing Filters}
Now that DDF are compared on a full stream, we observe how each of them behave when they act as a subfilter in our queueing scheme. The goal is to empirically prove the assumption made in Section~\ref{sub:optimal_ddf}, stating that the best (error-wise) DDF operating on a full stream leads to the best (error-wise) queueing filter operating on a sliding window. From Figure~\ref{fig:graph_n}, we observe that the best DDF for DDP on a full stream are, in order, QHT, SQF, Cuckoo, SQF.

For this, we reused the same filters are previously mentioned.
%\begin{itemize}
%	\item QHT subfilter \cite{10.1145/3297280.3297335}, 1 bucket per row, 3 bits per fingerprint. 
%	\item Cuckoo Filter \cite{Fan14}, adapted for streaming as recommanded by \cite{10.1145/3297280.3297335}, cells containing 1 element of 3 bits each
%	\item SQF \cite{Dut13}, 1 bucket per row, $r = 2$ and $r' = 1$
%	\item SBF \cite{Den06}, 2 bits per cell, 2 hash functions, targeted FPR of 0.02
%	\item A2
%\end{itemize}

We used a random stream of $10,000,000$ elements uniformly selected from an alphabet of size $|\Gamma| = 2^{18}$, a memory size $M=100,000$ bits, a sliding window of size $w = 10,000$. The error rates (defined as the sum of $\FNR^w$ and $\FPR^w$) are displayed in Figure~\ref{tab:comparing_queueing}. We see that in all cases, the QHT subfilter performs better or almost as well than any other subfilter, hence validating the heuristic assumption.

A very interesting observation is that, for $N$ the length of the stream, when $w \approx N/L$ we observe a decrease in the error rates. While being interesting on the theoretical side, in practice we usually assume that the stream is infinite, so we cannot rely on this fact for designing our filter. As such, the study of the phenomenon is left for future work.

\begin{figure}
	\input{graphs/compare_queues.tex}
	\caption{Error rate (times 100) of various queueing filters on various sliding window sizes, $M=100000$, $L=10$, $|\Gamma| = 2^{18}$.}\label{tab:comparing_queueing}
\end{figure}
%\begin{table}[]
%	\begin{tabular}{rccccc}
%			   & $w=1000$ & $w=5000$ & $w=10^4$ & $w=10^5$ & $w=10^6$ \\ \hline
%		QHT    & \textbf{9.84} 	  & \textbf{26.25}	 & \textbf{41.09}	 & \textbf{87.89}	  & 70.76      \\
%		Cuckoo & 84.99	  & 99.94	 & 99.95	 & 99.93	  & 99.80      \\
%		SQF    & 16.73 	  & 48.76	 & 69.25	 & 95.77 	  & \textbf{70.14}     \\
%		SBF    & 54.88 	  & 93.93	 & 97.33	 & 97.41	  & 71.17      \\
%	\end{tabular}
%	\caption{Error rate (times 100) of various queueing filters on various sliding window sizes, $M=100000$, $L=10$, $|\Gamma| = 2^{16}$. For each sliding window, the best performance is written in bold.}\label{tab:comparing_queueing}
%\end{table}

\todo

\subsection{Benchmark of the influence of $L$ parameter}\label{sub:L_bench}
In this section, we make experiments on the influence of $L$ on the error rate of the queueing filter. 
\subsection{Comparison of QHT and QHT-Queueing Filter}
In this section, we compare the efficiency of QHT and QHT-queueing filter. For this, we use both an artificial dataset, as well as a real one, on various sliding window sizes.




\section{Security}
%% Acknowledgements 
%\begin{acks} \end{acks}

%% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{qhtv2}

%% Appendix
%\appendix

\end{document}
