\input{preamble.tex}

\begin{document}

\title{Adapting Duplicate Filters to a Sliding Window Context}

%% Replace by \iffalse when submitting anonymously 
\iftrue 
	\author{Rémi Géraud-Stewart}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \state{France}
	} \email{remi.geraud@ens.fr}

	\author{Marius Lombard-Platet}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \country{France}
	}
	\affiliation{%
	\institution{Be-Studys}
	\city{Geneva} \country{Switzerland}
	} \email{marius.lombard-platet@ens.fr}

	\author{David Naccache}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \country{France}
	} \email{david.naccache@ens.fr}
\fi 

%% Abstract
\begin{abstract}
	A duplicate detection filter is an online data structure designed to efficiently detect repetitions (duplicates) in a stream, when only a limited amount of memory is available. A well-known data structure used in this context is the Bloom filter, which has been generalised in several ways to 
	improve its performances or strike specific tradeoffs. 
	
	When analysed in the limit of an infinite stream, error rates of duplicate filters turn out to be heavily constrained; as a consequence they appear to provide no advantage, asymptotically, over a random biased coin toss \cite{10.1145/3297280.3297335}.

	In this paper we consider a \enquote{windowed} variation on the duplicate detection problem
	that is better-behaved and thus lends itself to a more thorough 
	mathematical analysis. We show how existing filters can be adjusted to work
	in this new setting using a queuing construction. 
\end{abstract}

%% CCS XML
\begin{CCSXML}
	% Get code at http://dl.acm.org/ccs.cfm.
	% Please copy and paste the code
\end{CCSXML}

%% Keywords
\keywords{
	% TODO
}

\maketitle

\section{Introduction}
\subsection{Motivation}

We begin with the following, ideal problem: 
\begin{definition}[Duplicate detection problem, DDP] Given a stream $E_n = (e_1, e_2, \dotsc, e_n)$ and a \enquote{new} item $e^\star$, find whether $e^\star \in E_n$.\footnote{
	We do not consider here \emph{fuzzy} detection approaches \cite{10.5555/1287369.1287420,1410199,10.1109/ICDE.2011.5767865,10.1109/ICDE.2012.20,Monge97anefficient} that study situations where $e^\star$ belongs to $E_n$ when there exists a \enquote{similar enough} element in the stream.
}
	At every time increment, the new item is added to the stream, i.e., $E_{n+1} = E_n \mid e^\star$ where $\mid$ denotes concatenation. 
\end{definition}
Instances of this problem abound in computer science, with applications in file system indexation, database queries, network load balancing, network management \cite{10.5555/647912.740658}, in credit card fraud detection \cite{DBLP:journals/corr/abs-1709-08920}, phone calls data mining \cite{10.1145/347090.347094}, etc. A discussion about algorithms on large data streams can be found in \cite{10.1145/776985.776986}. 

In practice, additional constraints exist that we can capture with the following definition:
\begin{definition}[DDP with bounded memory]
	At every time step $n$, given $e^{\star}$ and a current state (dependent on history) of at most $M$ bits, solve the DDP for $E_n$ and $e^{\star}$.
\end{definition}
%Even when the stream is bounded, i.e. $\lim_{n\to\infty}|E_n| < \infty$, it may be impractical to perform a complete lookup and approximate solutions are desirable. 
One can prove \cite[Theorem 2.1]{10.1145/3297280.3297335} that perfect detection of duplicates in a stream with $U$ possible values requires $U$ bits of memory: even for modest situations (e.g. 64-bit nonces, corresponding to $2^{64}$ possible values) this is quickly impractical. As a result, we need a further relaxation of the DDP that allows for errors: 
\begin{definition}[Approximate DDP with bounded memory]
	Solve the DDP using at most $M$ bits of memory, with at most $\epsilon < 1/2$ mispredictions.\footnote{Given a classifier with $\epsilon > 1/2$, we obtain a classifier with $\epsilon < 1/2$ by reversing its output.} 
\end{definition}
Approximate duplicate detection has many real-life use cases, and can sometimes play a critical role, for instance in cryptographic schemes where all security and secrecy fall apart as soon as a random nonce is used twice, such as the ElGamal \cite{10.1007/3-540-39568-7_2} or ECDSA signatures. Other uses include improvements over caches \cite{4484874}, duplicate clicks \cite{10.1145/1060745.1060753}, and others.

On a side note, it is clear that the input distribution plays a central role regarding how efficiently the DDP can be solved. For instance, some deterministic streams may be expressed very compactly (such as the output of a PRNG with known seed) making the DDP relatively easy. Information-theoretically, if the source has $U$ bits of entropy then the situation is equivalent to having an $U$-bit, uniformly distributed input. 

Several algorithms have been proposed to solve the DDP, but
they \emph{all} face a phenomenon called \enquote{saturation} on large data streams \cite{10.1145/3297280.3297335}. Essentially, when saturation happens the algorithm performs no better than at random. This is problematic on two grounds: it makes the comparison of several algorithms difficult (since they all asymptotically behave in that fashion), and the unavoidable saturation ruins any particular design's merits.  

\subsection{Contributions}
To circumvent the aforementioned limitations, we introduce a variant of the DDP \emph{over a sliding window}, which enables the design of filters that avoids the saturation issue altogether (Section~\ref{sec:dudefisa}). We provide explicit upper bounds for the amount of memory necessary to solve this problem, and corresponding constructions.

We then introduce the \emph{queuing construction}, a generic approach to build windowed-DDP-friendly filters from existing filters (Sections~\ref{sec:queue} and \ref{sec:subfilter-select}). We analyse the queuing construction's error rates and parameters (Section~\ref{sec:optimising}), which are confirmed by benchmarks (Section~\ref{sec:bench}). 

Finally, we provide an analysis of the queuing construction's resistance to adversarial streams (Section~\ref{sec:adversarial}).

\subsection{Related work}\label{sub:related}
\cite{10.1145/1060745.1060753} studies a structure aking to the counting Bloom filter, in a \enquote{landmark-} and \enquote{jumping-} sliding window setting. Despite the name, these are different from the sliding window discussed in this paper.

Closer to our work, the idea of using subfilters may be found in the A2 filter's design \cite{Yoo10}. The A2 is built from two Bloom filters, a situation which we generalise and analyse generically in this paper.

\todo[continuer]

\subsection{Organisation}

\section{Duplicate Detection and Filter Saturation}
\label{sec:dudefisa}
\subsection{Notations and Initial Problem Statement}
Let us consider an alphabet $\Gamma$. A stream $E$ over $\Gamma$ is a (possibly infinite) set $E = (e_1, e_2, \dotsc)$, with each $e_i \in \Gamma$.

\begin{definition}[Duplicate and unseen element]
Let $E = (e_1, e_2, \dotsc)$ be a stream over $\Gamma$. Some element $e_j \in E$ is called a \emph{duplicate} if and only if there exists $i < j$ such that $e_i = e_j$. In that case we write $e_j \dup E$. Otherwise, $e_j$ is said to be \emph{unseen} and we write $e_j \notdup E$.
\end{definition}

\begin{definition}[Duplicate detection filter]
	A \emph{duplicate detection filter} (DDF) is a collection 
	$\mathcal F = (\mathcal S, \mathsf{Insert}, \mathsf{Detect})$ of a state\footnote{Here, $\mathcal M$ denotes all the memory states that can be reached by the filter.} $\mathcal S \in \mathcal M$ and two algorithms,
	\begin{itemize}
		\item $\mathsf{Detect}: \mathcal M \times \Gamma \to \{\DUPLICATE, \UNSEEN\}$;
		\item $\mathsf{Insert}: \mathcal M \times \Gamma \to \mathcal M$.
	\end{itemize}
	Informally, the $\mathsf{Detect}$ algorithm tentatively labels an element $e$ as duplicate or not, given the current state $\mathcal S$; whereas $\mathsf{Insert}$ takes as input the current state, an element $e$ to insert in the filter, and returns an updated memory state $\mathcal S'$. 
\end{definition}

We will usually consider the situations where the available memory is too small for perfect detection, i.e., $|\mathcal S| \ll |\Gamma|$.

\begin{definition}[False positive, false negative]
	Let $e \dup E$ (resp. $e \notdup E$) and $\mathcal F$ be a filter. We say that $e$ is a \emph{false positive} (resp. \emph{false negative}) when $\mathcal F.\mathsf{Detect}(e) = \DUPLICATE$ (resp. $\UNSEEN$).
\end{definition}

\begin{definition}[False positive rate, false negative rate]
	Let $E$ be a stream, $\mathcal F$ be a filter. The \emph{false positive rate} (resp. \emph{false negative rate}) of $\mathcal F$ over the first $i < n$ elements, denoted $\FPR_i$ (resp. $\FNR_i$), is defined as the ratio of false positives divided over the number of unseen elements in $(e_1, \dotsc, e_i)\subset E$ (resp. the ratio of false negative divided by the number of duplicate elements in that subset).
\end{definition}

However, it is more practical to work with another value, namely, the probability that after $m$ insertions, the element $e^\star$ is a false positive (resp. false negative). 

\begin{definition}[Probability of false positive, of false negative]
	\label{def:pfp-pfn}
	Let $E = (e_1, \dotsc)$ be a stream, $\mathcal F$ be a filter. The \emph{probability of false positive} (resp. \emph{probability of false negative}) of $\mathcal F$ after $i$ elements, denoted $\FP_i$ (resp. $\FN_i$), is defined as the probability that $e_{i+1}$ is a false positive (resp. false negative).
\end{definition}

One can observe that $\FPR_n = \frac{1}{n}\sum_{i=1}^n \FP_i$, and similarly for $\FNR_n$.

\subsection{Filter Saturation and Sliding Windows}\label{sec:filsat}
Unfortunately, efficient duplicate detection over an infinite stream has been identified as an unsolvable problem \cite{10.1145/3297280.3297335}. As a matter of fact, as the number of bits of information of the stream grow to infinity, the filter can only remember an ever diminishing proportion of those bits, eventually having no advantage compared to a filter answering randomly.

In other terms, any DDF is asymptotically as good as a random guess:
	\begin{theorem}[\cite{10.1145/3297280.3297335}]\label{thm:sat_u}
	Let $\FNR_\infty$ and $\FPR_\infty$ be the asymptotic false negative and false positive rates of a filter of $M$ bits of memory (i.e. the FNR and FPR on a stream of size going to infinity).
	
	If $|\Gamma| \gg M$, then $\FNR_\infty + \FPR_\infty = 1$, which characterises random filters (i.e. filters answering randomly to any query).
\end{theorem}

One way out of this situation is to lower our expectations and work on a sliding window:

\begin{definition}[Sliding window DDP]
	Given a stream $E_n$ of fixed size $w$, with $E_n = (e_{n-w+1}, \dotsc, e_n)$ and an item $e^\star$, find whether $e^\star \in E_n$.
\end{definition} 

We therefore introduce new definitions suited to that setting:

\begin{definition}[Duplicate element, unseen element over a sliding window]
	Let $E = (e_1, e_2, \dotsc)$ be a stream over $\Gamma$, let $w\in \mathbb N$, an element $e_j \in E$ is called a \emph{duplicate} in $E$ over the sliding window of size $w$ if and only if there exists $j$ such that $j - w \leq i < j$  and $e_i = e_j$. In this case we write $e_j \wdup E$. Otherwise, $e_j$ is said to be \emph{unseen} and we write $e_j \notwdup E$.
\end{definition}

\begin{definition}[False positive, false negative over a sliding window]
	Let $w \in \mathbb N$, $e \notwdup E$ (resp. $e \wdup E$) and let $\mathcal F$ be a DDF. We say that $e$ is a \emph{false positive} (resp. \emph{false negative}) over $w$ if $\mathsf{Detect}(e) = \DUPLICATE$ (resp. $\UNSEEN$).
\end{definition}

\begin{definition}[False positive rate, false negative rate over a sliding window]
	Let $E$ be a stream, $\mathcal F$ be a DDF, $w \in \mathbb N$. The \emph{false positive} (resp. \emph{false negative}) rate of $\mathcal F$ over the first $i < n$ elements and the sliding window $w$, noted $\FPR^w_i$ (resp. $\FNR^w_i$), is defined as the number of false positives over $w$ divided by the number of unseen elements in $E$ over $w$ (resp. the number of false negative over $w$ divided by the number of duplicate elements over $w$).
\end{definition}


\subsection{Bounds on the Sliding Window DDP}\label{sub:opt_solutions}

Our goal is to minimise both the false positive and false negative rates of a DDF, given a fixed and limited amount of memory. Before going into details, we first explore the limits of the problem, i.e., when perfect or very performant solutions exist.

\begin{theorem}
	For $M \geq w (\log_2(w) + 1) \log_2(|\Gamma|)$, the sliding window DDP can be solved exactly (with no errors) in constant time.
\end{theorem}

\begin{proof}
	We explicitly construct a DDF that performs the detection. Storing all $w$ elements in the sliding window takes $w\log_2(|\Gamma|)$ memory, using a FIFO queue $Q$; however 
	lookup has a worst-time complexity of $O(w)$. 
	
	We therefore rely on an
	ancillary data structure for the sake of quickly answering lookup questions.
	Namely we use a dictionary $D$ whose keys are elements from $\Gamma$ and values are counters.
	
	Whe an element $e$ is inserted in the DDF, $e$ is stored and $D[e]$ is incremented (if the key $e$ did not exist in $D$, it is created first, and
	$D[e]$ is set to $1$). In order to keep the number of stored elements to $w$, 
	we discard the oldest element $e_\text{last}$ in $Q$. As we do so, we also decrement $D[e_\text{last}]$, and if $D[e_{last}] = 0$ the key is deleted from $D$. The whole insertion procedure is therefore performed in constant time.
	
	Lookup of an element $e^\star$ is simply done by looking whether the key $D[e^\star]$ exists, which is done in constant time.
	
	The size of the queue is $w\log_2 |\Gamma|$, the size of the dictionary is $w \log_2 |\Gamma| \log_2 w$ (as the dictionary cannot have more than $w$ keys as the same time, and a counter cannot go over $w$, thus being less than $\log_2 w$ bits long). Thus at most $w (\log_2(w) + 1) \log_2(|\Gamma|)$ bits are necessary for this DDF to work.

	Finally this filter does not make any mistake, as the dictionary $D$ keeps an exact tract of how many times each element was present in the sliding window.
\end{proof}
The dependence on $\log_2 |\Gamma|$ can be droped, at the cost of allowing 
errors:
\begin{theorem}
	Let $w \in \mathbb N$. Let $M \simeq 2w\log_2w$, then the sliding window DDP can be solved with almost no error using $M$ bits of memory.
	
	More precisely, it is possible to create a filter of $M$ bits with an FNR of $0$, and an FPR of $1 - (1-\frac1{w^2})^w \sim \frac 1w$, and a time complexity of $O(w)$.
	
	Using $M \simeq 5w\log_2 w$ bits of memory, a constant-time filter with the same error rate can be constructed.
\end{theorem}

\begin{proof}
	Here again we explicitly construct the filters that attain the theorem's bounds.

	Let $h$ be a hash function with codomain $\{0,1\}^{2 \log_2 w}$.
The birthday theorem, as summed up in \cite{10.1007/3-540-45708-9_19}, states that for a hash function $h$ over $a$ bits, one must on average collect $2^{a/2}$ input-output pairs before obtaining a collision. Therefore $2^{(2 \log_2 w) / 2} = w$ hash values $h(e_i)$ can be computed before having a $50\%$ probability of a collision (here, a collision is when two distinct elements of the stream $e_i, e_j$ with $i \neq j, e_i \neq e_j$ have the same hash, i.e. $h(e_i) = h(e_j)$). The 50\% threshold we impose on $h$ is arbitrary but nonetheless practical.

Let $\mathcal F$ be the following DDF: the filter's state consists in a queue of $w$ hashes, and for each new element $e$, $\mathsf{Detect}(e)$ returns \DUPLICATE if $h(e)$ is present in the queue, \UNSEEN otherwise. $\mathsf{Insert}(e)$ appends $h(e)$ to the queue before popping the queue. 

There is no false negative, and a false positive only happens if the new element to be inserted collides with at least one other element, which happens with probability $1 - (1 - \frac 1{2^{2\log_2w}})^{w} = 1 - (1-\frac1{w^2})^w$, hence an FNR of $0$ and a FPR of $1 - (1-\frac1{w^2})^w$.
The queue stores $w$ hashes, and as such requires $w \cdot 2\log_2 w$ bits of memory.

Note that this solution has a time complexity of $O(w)$. Using an additional dictionary, as in the previous proof, we get a filter with an error rate of about $\frac 1w$ and constant time for insertion and lookup, using $5w\log_2 w$ bits of memory.
\end{proof}

When $\log_2|\Gamma| > 2 \log_2 w$ (which is the case of most practical use cases) this DDF outperforms the naïve strategy\footnote{The naïve strategy consisting of storing the $w$ elements of the sliding window, requiring $w \log_2|\Gamma|$ bits of memory.}, both in terms of time and memory.


Hence, very good solutions for situations where $M \approx 5w\log_2 w$ already exist, but efficient solutions and lower bounds on the error rates for smaller amounts of memory are still to be found.


\subsection{DDP vs Sliding Window DDP}
The sliding window variant of DDP may seem to be a slight weakening of the original problem. Remarkably, it turns out that DDFs which perform well for the DDP do rather poorly for the sliding window DDP. A literature review collects the following DDF constructions: A2 filters \cite{Yoo10}, Stable Bloom Filters (SBF) \cite{Den06}, Quotient Hash Tables (QHT) \cite{10.1145/3297280.3297335}, Streaming Quotient Filters (SQF) \cite{Dut13}, Block-decaying Bloom Filters (b\_DBF) \cite{She08}, and a slight variation of Cuckoo Filters \cite{Fan14} suggested by \cite{10.1145/3297280.3297335}. The structure proposed in \cite{10.1145/1060745.1060753} is not suited to either sliding window or no sliding window DDP, as they work on what they call `landmark` sliding window, which consists of a zero-resetting of the memory at some user-defined epochs.

Of these, only the A2 and b\_DBF were designed in a way immediately applicable to the sliding window DDP. The others do not account for a finite-length sliding window, and therefore cannot be adjusted as a function of $w$. Another remarkable (but experimental) observation is that these DDFs' false positive rate does not decrease to 0 when the sliding window becomes small.

\todo[Expliquer ?]


\section{Queuing filters}\label{sec:queue}
In this section, we describe the queuing construction, which produces a sliding window DDF from any DDF. We first give the description of the setup, before studying the theoretical error rates. 
A scheme describing our structure is detailed in Figure~\ref{fig:scheme}.

\subsection{The queuing construction}

\paragraph{Principle of operation.}
Let $\mathcal F$ be a DDF. Rather than allocating the whole memory to $\mathcal F$, we will create $L$ copies of $\mathcal F$, each using a fraction of the available memory. Each of these \emph{subfilters} as a limited timespan, and is allowed up to $c$ insertions. The subfilters are organised in a queue.

When inserting a new element in the queuing filter, it is inserted in the topmost subfilter of the queue. After $c$ insertions, a new empty filter is added to the queue, and the oldest subfilter is popped and erased.

As such, we can consider that each subfilter operates on a sub-sliding window of size $c$, which makes the overall construction a DDF operating over a sliding window of size $w = cL$.

\begin{figure}
	\input{fig/queuef.tex}
	\caption{Architecture of the queuing filter. The filter is composed of $L$ subfilters $\mathcal F$, each containing up to $c$ elements. Once the newest subfilter has inserted $c$ elements in its structure, the oldest one is expired. As such, it is dropped and a new one is created and put under population at the beginning of the queue. In this example, the sub-sliding window of $\mathcal F_1$ is $(e_{m-2}, e_{m-3}, e_{m-4})$.} \label{fig:scheme}
\end{figure}

\paragraph{Insertion and lookup.}
The filter returns \DUPLICATE if and only if at least one subfilter does. Insertion is a simple insertion in the topmost subfilter.

\paragraph{Queue update.}
After $c$ insertions, the last filter of the queue is dropped, and a new (empty) filter is appended in front of the queue.

\paragraph{Pseudocode.}
We give a brief pseudocode for the queuing filter's functions \textsf{Lookup} and \textsf{Insert}, as well as a \textsf{Setup} function for initialisation, in Algorithm~\ref{alg:queuing}. We introduced for simplicity a constructor $\mathcal F.\textsf{Setup}$ that takes as input an integer $M$ and outputs an initialized empty filter $\mathcal F$ of size at most $M$. Here \texttt{subfilters} is a FIFO that has a \texttt{pop} and \texttt{push\_first} operations, which respectevely removes the last element in the queue or inserts a new item in first position.\footnote{Two standard ways to implement this are a double-ended queue (deque) and a ring buffer.}   

\begin{algorithm}[]
	\caption{Queuing Filter Setup, Lookup and Insert}\label{alg:queuing}
	
	\begin{algorithmic}[1]
	\Function{Setup}{$\mathcal F, M, L, c$}
		\Comment $M$ is the available memory, $\mathcal F$ the subfilter structure, $L$ the number of subfilters and $c$ the number of insertions per subfilter
		\State subfilters $\gets \emptyset$
		\State counter $\gets 0$
		\State $m \gets \lfloor M/L \rfloor$
		\For{$i$ from $0$ to $L-1$}
			\State subfilters.push\_first$(\mathcal F.\mathsf{Setup}(m))$
		\EndFor
		\State \textbf{store} (subfilters, $L$, $m$, counter)
		\State\Return filter
	\EndFunction 
	\item[]
	\end{algorithmic}

	\begin{algorithmic}[1]
	\Function{Lookup}{$e$}
		\For{$i$ from $0$ to $L-1$}
			\If{subfilters[i]$.\mathsf{Lookup(e)}$}
				\State \Return $\DUPLICATE$
			\EndIf
		\EndFor
		\State \Return \UNSEEN
	\EndFunction
	\item[]
	\end{algorithmic}

	\begin{algorithmic}[1]
	\Function{Insert}{$e$}
		\State subfilters[0]$.\mathsf{Insert}(e)$
		\State counter $\gets$ counter + 1 
		\If{counter == $c$}
			\State subfilters.pop()
			\State subfilters.push\_first($\mathcal F.\textsf{Setup}(m)$)
		\EndIf
	\EndFunction
	\end{algorithmic}
\end{algorithm}

\subsection{Error rate analysis}
The queuing filter's properties can be derived from the subfilters'. False positive and false negative rates are of particular interest. In this section we consider a queuing filter $\mathcal Q$ with $L$ subfilters of type $\mathcal F$ and 
capacity $c$ (which means that the last subfilter is dropped after $c$ insertions).

\paragraph{Remark.}
	\label{subsub:number_elements}
	By definition, after $c$ insertions the last subfilter is dropped.
Information-theoretically, this means that all the information related to the elements inserted in that subfilter has been lost, and there are $c$ such elements by design. 
Therefore, in the steady-state regime, the queuing filter holds information about at least $c(L-1)$  elements (immediately after deleting the last subfilter) and at most $cL$ elements (immediately before this deletion). 

As a result, if $w < cL$, the queuing filter can hold information about \emph{more than $w$ elements}.

\subsubsection{Probability of False Positive}

\begin{theorem}
	\label{thm:queue-pfp}
	 Let $\FP_{\mathcal Q, m}^w$ be the probability of false positive (def. \ref{def:pfp-pfn}) of $\mathcal Q$ after $m > w$ insertions, over a sliding window of size $w = cL$, we have:
	\begin{equation}
		\label{eq:queue-pfp}
		\FP_{\mathcal Q, m}^w = 1 - \left(1 - \FP_{\mathcal F,c}\right)^{L-1} \left(1 - \FP_{\mathcal F, m \bmod c }\right)
	\end{equation}
	where $\bmod$ is the modulo operator and 
	 $\FP_{\mathcal F,m}$ be the probability of false positive a subfilter $\mathcal F$ after $m$ insertions.
\end{theorem}

\begin{proof}
	Let $E = (e_1, \dotsc, e_m, \dotsc)$ be a stream and $e^\star \notwdup E$. 
	
	Therefore, $e^\star$ is a false positive if and only if at least one subquery $\mathcal F_i.\mathsf{Lookup(e^\star)}$ returns \DUPLICATE. Conversely, $e^\star$ is \emph{not} a false positive when all subqueries $\mathcal F_i.\mathsf{Lookup(e^\star)}$ return \UNSEEN, i.e., when $e^\star$ is not a false positive for each subfilter.
	
	Each subfilter has undergone $c$ insertions, safe for the first subfilter which has only undergone $m \bmod c$, we immediately get Eq.~(\ref{eq:queue-pfp}).
\end{proof}

\paragraph{Remark.} In the case $w < cL$, as mentioned previously, there is a non-zero probability that $e^\star$ is in the last subfilter's memory, despite not belonging to the sliding window. 
	
Assuming a uniformly random input stream, and writing $\delta = cL - w$, the probability that $e^\star$ has occurred in $\{e_{m-cL}, \dotsc e_{m-w+1}\}$ is $1-\left(1-\frac1{|\Gamma|}\right)^\delta$. For large $\Gamma$ (as is expected to be the case in most applications), this probability is about $\frac{\delta}{|\Gamma|} \ll 1$. Hence, we can neglect the probability that $e^\star$ is present in the filter, and we consider the result of Theorem~\ref{thm:queue-pfp} to be a very good approximation even when $w < cL$.

\subsubsection{Probability of False Negative}\label{sub:fnr}
\begin{theorem}
	\label{thm:queue-pfn}
	Let $\FN_{\mathcal Q, m}^w$ be the probability of false negative of $\mathcal Q$ after $m > w$ insertions on a sliding window of size $w = cL$, we have
	\begin{equation}
	\FN_{\mathcal Q, m}^w 
	=  u_c^{L-1} u_{m \bmod c}
	\end{equation}
	where $\bmod$ is the modulo operator, and we have introduced the short-hand notation $u_\eta= p_{\eta}\FN_{\mathcal F,\eta} + \left(1-p_{\eta}\right)\left(1-\FP_{\mathcal F,\eta}\right)$ where 
	$\FN_{\mathcal F,\eta}$ (resp. $\FP_{\mathcal F, \eta}$) is the probability of false negative (resp. false positive) of the subfilter $\mathcal F$ after $\eta$ insertions, and
	\begin{equation*}
		p_\eta = \frac{1-\left(1-\frac 1{|\Gamma|}\right)^\eta}{1-\left(1-\frac 1{|\Gamma|}\right)^w} \approx \frac{\eta}{w}.
	\end{equation*}
\end{theorem}

\begin{proof}
	Let $E = \{e_1, \dotsc, e_m, \dotsc\}$ be a stream, let $w$ be a sliding window and let $e^\star \wdup E$. 
	
	Then $e^\star$ is a false negative if and only if all subfilters $\mathcal F_i$ answer $\mathcal F_i.\mathsf{Detect}(e^\star) = \UNSEEN$. There can be two cases:
	\begin{itemize}
		\item $e^\star$ is present in $\mathcal F_i$'s sub-sliding window;
		\item $e^\star$ is not present in $\mathcal F_i$'s sub-sliding window.
	\end{itemize}
In the first case, $\mathcal F_i.\mathsf{Detect}(e^\star)$ returns \UNSEEN if and only if $e^\star$ is a false negative for $\mathcal F_i$. This happens with probability $\FN_{\mathcal F,c}$ by definition, except for $\mathcal F_0$, for which the probability is $\FN_{\mathcal F, m\bmod c}$.

In the second case, $\mathcal F_i.\mathsf{Detect}(e^\star)$ returns \UNSEEN if and only if $e^\star$ is not a false positive for $\mathcal F_i$, which happens with probability $1-\FP_{\mathcal F,c}$, execpt for $\mathcal F_0$, for which the probability is $1-\FP_{\mathcal F, m\bmod c}$.

Finally, each event is weighted by the probability $p_c$ that $e^\star$ is in $\mathcal F_i$'s sub-sliding window:
\begin{align*}
	p_c 
	& = \Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window | } e^\star \wdup E]\\
	& = \frac{\Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window } \cap e^\star \wdup E]}{\Pr[e^\star \wdup E]}\\
	& = \frac{\Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window}]}{\Pr[e^\star \wdup E]}\\
	& = \frac{1 - \Pr[\text{$e^\star$ is not in }\mathcal F_i \text{ sub-sliding window }]}{1 - \Pr[e^\star \notwdup E]}\\
	& = \frac{1 - \left(1 - \frac 1{|\Gamma|}\right)^c}{1 - \left(1-\frac{1}{|\Gamma|}\right)^w}
\end{align*}
This concludes the proof.
\end{proof}
\paragraph{Remark.} As previously, the effect of $w < cL$ is negiglible for all practical purposes and Theorem~\ref{thm:queue-pfn} is considered a good approximation in that regime. 

\subsection{FNR and FPR}
From the above expressions we can derive relatively compact explicit formulas for the queuing filter's FPR and FNR when $m = cn$ for $n$ a positive integer.
\begin{theorem}
	Let $\FPR_{\mathcal Q, m}^w$ be the false positive rate of $\mathcal Q$ after $m=cn > w$ insertions on a sliding window of size $w = cL$, we have
	\begin{equation}
	\FPR_{\mathcal Q, cn}^w = 1 -  \frac{(1 - \FP_{\mathcal F, c})^{L-1}}{c}\sum_{\ell = 0}^{c-1} (1-\FP_{\mathcal F, \ell}).
	\end{equation}
\end{theorem}
\begin{proof}

\begin{align*}
	\FPR_{\mathcal Q, cn}^w
	& = \frac{1}{cn} \sum_{k=1}^{cn} \FP_{\mathcal Q, k}^w 
	 = \frac{1}{cn} \sum_{k=1}^{n} \sum_{\ell = 0}^{c-1} \FP_{\mathcal Q, k + \ell}^w 
	 = \frac1c \sum_{\ell = 0}^{c-1} \FP_{\mathcal Q, \ell}^w \\
	& = \frac1c \sum_{\ell = 0}^{c-1}  1 - (1 - \FP_{\mathcal F, c})^{L-1}(1 - \FP_{\mathcal F}, \ell) \\
	& = 1 - \frac1c(1 - \FP_{\mathcal F, c})^{L-1}\sum_{\ell = 0}^{c-1} (1-\FP_{\mathcal F, \ell})
\end{align*}
\end{proof}

\begin{theorem}
	Let $\FNR_{\mathcal Q, m}^w$ be the false negative rate of $\mathcal Q$ after $m=cn > w$ insertions on a sliding window of size $w = cL$, we have
	\begin{equation}
	\FNR_{\mathcal Q, cn}^w = \frac{u_c^{L-1}}{c}  \sum_{\ell = 0}^{c-1}u_\ell.
	\end{equation}
\end{theorem}

\begin{proof}
\begin{align*}
	\FNR_{\mathcal Q, cn}^w 
	& = \frac{1}{cn} \sum_{k=1}^{cn} \FN_{\mathcal Q, k}^w 
	 = \frac{1}{cn} \sum_{k=1}^{n} \sum_{\ell = 0}^{c-1} \FN_{\mathcal Q, k + \ell}^w 
	= \frac1c \sum_{\ell = 0}^{c-1} \FN_{\mathcal Q, \ell}^w \\
	& = \frac1c \sum_{\ell = 0}^{c-1} u_c^{L-1} u_{\ell} 
	 = \frac{u_c^{L-1}}{c}  \sum_{\ell = 0}^{c-1}u_\ell
\end{align*}
\end{proof}
As for the probabilities, the expressions derived above for the FNR and FNR are valid to first order in $(w - cL)/|\Gamma|$, i.e. they are good approximations even when $w \approx cL$. 

\section{Optimising queuing filters}
\label{sec:optimising}

The parameter $L$ determines how many subfilters appear in the queuing construction. Summing up the false positive and false negative rates, we have a total error rate 
\begin{equation}
	\operatorname{ER}_{\mathcal Q, cn}^{w}
	= 1 - \alpha \beta^{L-1} + \alpha' \beta'^{L-1}
\end{equation}
where $\beta = 1 - \FP_{\mathcal F, c}$, $\beta' = u_c$, 
$\alpha = \frac1c \sum_{\ell = 0}^{c-1}1 - \FP_{\mathcal F, \ell}$ and $\alpha' = \frac1c \sum_{\ell = 0}^{c-1} u_\ell$ only depend on $c$ and the choice of subfilter type $\mathcal F$. Differentiating with respect to $L$ and equating to zero gives an optimum
\begin{equation*}
	L = -\log((\alpha \beta' \log(\beta))/(\alpha'\beta \log(\beta')))/\log(\beta/\beta').
\end{equation*} 

\todo[À vérifier la suite je suis pas convaincu encore]
\begin{theorem}
	In first approximation, a low value of $L$ for constant value $cL$ leads to a higher false negative probability for the queuing filter.
\end{theorem}

\begin{proof}
As a matter of fact, on average the queuing filter consists of $L-1$ subfilters with $c$ insertions, and one subfilter with $\frac c2$ insertions. As such, we can say that the queuing filter is a representation of the last $(c-\frac12) L$ last elements. 

Let us consider $e^\star \wdup E$. If, in the sliding window, all duplicates only happened at least $(c-\frac12)L$ epochs ago (which happens with probability $\left(1-\frac1{|\Gamma|}\right)^{(c-\frac12)L}$), we know that no duplicates happened in any subfilter's sliding window. By using the same reasoning as Theorem~\ref{thm:queue-pfn}'s proof, we get a probability of false negative of around $\left[0 + \left(1-\frac1{|\Gamma|}\right)^{(c-\frac12)L}(1-\FP_{\mathcal F,c})\right]^L$. Given that $cL \approx w$ which is a constant, we get that the probability of false positive mostly varies, in this case, with $\left(1-\FP_{\mathcal F,c}\right)^L$. 

Hence, the lower $L$ is, the higher the probability of false negative will be.

The approximations made in this proof are backed by experimental results.
\end{proof}


\section{Queuing filters from existing DDFs}
\label{sec:subfilter-select}
\label{sec:optimal_ddf}
Our queuing construction relies on a choice of subfilters. A first observation 
is that we may assume that all subfilters can be instances of a single 
DDF design (rather than a combination of different designs).

Indeed, a simple symmetry argument shows that a heterogenous selection of subfilters is always worse than a homogeneous one: the crux is that all subfilters play the same role in turn. Therefore we lose nothing by replacing atomically one subfilter by a more efficient one. Applying this to each subfilter we end up with a homogenous selection.

It remains to decide which subfilter construction to choose. 
When comparing different DDFs, one interesting metric in our context is how fast
such filters saturate: indeed, in the original DDP setting, all filters eventually saturate \cite{10.1145/3297280.3297335}, but they may reach that state more or less quickly. The following result gives the behaviour of a perfect filter regarding this question:

\begin{theorem}
	Let $E$ be a stream of $n$ elements uniformly selected from an alphabet of size $|\Gamma|$. For any DDF using $M$ bits of memory, the error probability $ER_n = \FP_n + \FN_n$ satisfies
	\begin{equation}
		ER_n \geq 1 - \frac{1 - \left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n} 
	\end{equation}
	for any $n > M$.
\end{theorem}
	In particular, the asymptotic error rate $ER_\infty$ satisfies $ER_\infty \geq \left(1 - \frac{1}{|\Gamma|}\right)^{M} \approx 1 - M/|\Gamma|$.

\begin{proof}
	By definition, a perfect filter has the lowest possible error rate.
	With $M$ bits of memory, a perfect filter can store at most $M$ elements in memory \cite[Theorem 2.1]{10.1145/3297280.3297335}. Up to reordering the stream, without loss of generality because it is random, we may assumethat the filter stores the $M$ last elements of the stream: any other strategy cannot yield a strictly lower error rate.
	
	If an element is already stored in the filter, then the optimal filter will necessarily answer \DUPLICATE. On the other hand, if the element is not in memory, a perfect filter can choose to answer randomly. Let $p$ be the probability that a filter answers \DUPLICATE when an element is not in memory. An optimal filter will lower the error rate of any filter using the same strategy with a different probability.
	
	An unseen element, by definition, will be unseen in the $M$ last elements of the stream, and hence will not be in the filter's memory, so the filter will return \UNSEEN with probability $1-p$. For this reason, this filter has an FP probability of $p$.
	
	On the other hand, $e^\star \dup E$ is classified as \UNSEEN if and only if it was not seen in the last $M$ elements of the stream, and the filter answers \UNSEEN. Let $D$ be the event \enquote{\emph{There is at least one duplicate in the stream}} and $C$ be the event \enquote{\emph{There is a duplicate of $e^\star$ in the $M$ previous elements of the stream}}. Then $e^\star$ triggers a false negative with probability
	\begin{align*}
	\FN_n &= (1 - \Pr[C | D])(1-p) =  \left(1 - \frac{\Pr[C \cap D]}{\Pr[D]}\right)(1-p)\\
	& = \left(1 - \frac{\Pr[C]}{\Pr[D]}\right)(1-p) =  \left(1 - \frac{1 - \Pr[\bar C]}{1 - \Pr[\bar D]}\right)(1-p) \\
	\FN_n &=  \left(1 - \frac{1 - \left(1 - \frac 1{|\Gamma|}\right)^M}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}\right)(1-p)
	\end{align*}
	Hence, the error probability of the filter is
	\begin{align*}
		ER_n 
		& = \FN_n + p \\
		& =  \left(1 - \frac{1 - \left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}\right)(1-p) + p \\
		& = 1 - \frac{1-\left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}(1-p),
	\end{align*}
	which is minimized when $p = 0$. 
	\end{proof}

Note, as highlighted in the proof, that this bound is \emph{not tight}: better bounds may exist, the study of which we leave as an open question for future work. 

The results of an experimental comparison of different DDFs (details about the benchmark are given in Section~\ref{sub:saturation}) are summarizes in Figure~\ref{fig:graph_n}. It appears that the two most efficient filters (in terms of saturation rate) are the QHT \cite{10.1145/3297280.3297335} and the A2 \cite{Yoo10}.

As noted in Section~\ref{sub:related}, A2 filters are a primitive form of a queuing filter (with $L=2$ and Bloom filters as subfilters) and thus were not included in this benchmark, as we focused on adapting filters designed for DDP to the sliding window DDP. This raises the question of whether recursive constructions (queuing filters of queuing filters) have particularly interesting properties, which we leave open for future work. 

%In the experiments we use QHT for the benchmarking of the queuing construction.

\section{Experiments and Benchmarks}\label{sec:bench}

This section provides details and additional information on the benchmarking experiments run to validate the above analysis. All code is accessible online and will be disclosed after peer review.

Our benchmarks cover two aspects: 
\begin{enumerate}
\item A measure of DDFs' saturation rate, in the original DDP setting (i.e. without a sliding window), which is used to choose a subfilter design;
\item A measure of the effect of queuing on error rates, in the sliding window DDP setting. Namely, we compate QHT and a queuing QHT.
\end{enumerate}
An important note is that when the sliding window is small compared to the filter's memory size, optimal or nearly optimal solutions can be implemented (see Section~\ref{sub:opt_solutions}). This is practical for smaller values because we can measure exactly a filter's response compared to the \enquote{ground truth}. However, this is no longer possible for very large parameters as it would require too much memory for keeping track of the duplicates.

\subsection{Saturation Resistance of DDFs}\label{sub:saturation}
We evaluate the saturation rate for several DDFs, in the original DDP setting (without sliding window). Parameters are chosen to yield equivalent memory footprints and agree with \cite{10.1145/3297280.3297335}, namely:
%
\begin{itemize}
	\item QHT \cite{10.1145/3297280.3297335}, 1 bucket per row, 3 bits per fingerprint.
	\item SQF \cite{Dut13}, 1 bucket per row, $r = 2$ and $r' = 1$
	\item Cuckoo Filter \cite{Fan14}, cells containing 1 element of 3 bits each
	\item Stable Bloom Filter (SBF) \cite{Den06}, 2 bits per cell, 2 hash functions, targeted FPR of 0.02
	\item $A2$ Filter \cite{Yoo10}, targeted FPR of $0.1$ on the sliding window.
%	\item Block-Decaying Bloom Filter (b\_DBF) \cite{She08}, sliding window of 6000 elements.
\end{itemize}
%
These filters are run against a stream of uniformly sampled elements from an alphabet of $2^{26}$ elements. This results in around 8\% duplicates (up to statistical fluctuations) amongst the 150 000 000 elements in the longest stream used. As a comparison, we also run the same benchmark against a stream obtained from crawling data, the same one as \cite{10.1145/3297280.3297335}, a real-world source which is highly non-uniform --- note that in this case, our lower bound on the error rate does not apply, owing to the stream being compressible. In that stream, around 10\% were duplicates amongst the 150 000 000 elements. Results are plotted in Figure~\ref{fig:graph_n}.

\begin{figure*}[!ht]
	\begin{tikzpicture}
	\begin{groupplot}[
	group style = {
		group size = 2 by 1,
		horizontal sep = 35pt,
	},
	axis lines = left,
	ylabel near ticks,
	width = 0.4\textwidth,
	height = 0.27\textwidth,
	]
	\input{er_n_artificial}
	\input{er_n_real}
	%\input{10e6_sum_zipf}
	\end{groupplot}
    \path (top)--(bot) coordinate[midway] (group center);
%	\node[above,rotate=90] at (group center -| current bounding box.west) {Error rate (\%)};
	\node[right=1em,inner sep=0pt] at(group center -| current bounding box.east) {\pgfplotslegendfromname{legend_n}};
	\end{tikzpicture}

%	\input{er_n_artificial}
	\caption{Error rate (times 100) of DDFs of 1Mb, over a real and artificial stream, depending on the size of the stream, without any sliding window. Hatched area represents over-optimal (impossible) values.}\label{fig:graph_n}
\end{figure*}

The best results are given by the following filters, in order: QHT, SQF, Cuckoo and SBF. We also observe that QHT and SQF have error rates relatively close to the lower bound, hence suggesting that these filters are close to optimality.

\subsection{Benchmarking Queuing Filters}
We now use the queuing construction on the DDFs discussed previously. In Section~\ref{sec:optimal_ddf} we suggested the heuristic that the DDFs with the least saturation rate in the DDP would yield the best (error-wise) queuing filter for the sliding window DDP. This heuristic is supported by results, summarized in Figure~\ref{tab:comparing_queuing}. For this benchmark we used the following parameters: uniform stream from an alphabet of size $|\Gamma| = 2^{18}$, memory size $M=100,000$ bits, sliding window of size $w = 10,000$, and we measure the error rate (sum of $\FNR^w$ and $\FPR^w$).

A very interesting observation is that $Lw$ approaches the size of the stream, there is a drop in the error. This is an artifact due to the finite size of our simulations; the stream should be considered infinite, and this drop disappears as the simulation is run for longer (see Appendix~\ref{app:finite}). This effect also alters the error rates for smaller window sizes, albeit much less, and we expect that filter designers care primarily about the small window regime. Nevertheless a complete understanding of this effect would be of theoretical interest, and we leave the study of this phenomenon for future work.

\begin{figure}[!ht]
	\input{graphs/compare_queues.tex}
	\caption{Error rate (times 100) of various queuing filters on various sliding window sizes, $M=100000$, $L=10$, $|\Gamma| = 2^{18}$.}\label{tab:comparing_queuing}
\end{figure}
%\begin{table}[]
%	\begin{tabular}{rccccc}
%			   & $w=1000$ & $w=5000$ & $w=10^4$ & $w=10^5$ & $w=10^6$ \\ \hline
%		QHT    & \textbf{9.84} 	  & \textbf{26.25}	 & \textbf{41.09}	 & \textbf{87.89}	  & 70.76      \\
%		Cuckoo & 84.99	  & 99.94	 & 99.95	 & 99.93	  & 99.80      \\
%		SQF    & 16.73 	  & 48.76	 & 69.25	 & 95.77 	  & \textbf{70.14}     \\
%		SBF    & 54.88 	  & 93.93	 & 97.33	 & 97.41	  & 71.17      \\
%	\end{tabular}
%	\caption{Error rate (times 100) of various queuing filters on various sliding window sizes, $M=100000$, $L=10$, $|\Gamma| = 2^{16}$. For each sliding window, the best performance is written in bold.}\label{tab:comparing_queuing}
%\end{table}

\subsection{The number of subfilters}\label{sub:L_bench}
\todo[À faire]

\subsection{Comparison of QHT and QHT-Queuing Filter}
\todo[À faire]
In this section, we compare the efficiency of QHT and QHT-queuing filter. For this, we use both an artificial dataset, as well as a real one, on various sliding window sizes.

\section{Adversarial Resistance}
\label{sec:adversarial}

As DDFs have numerous security applications, we now discuss the queuing construction from an adversarial standpoint. We consider an adversarial game in which the attacker wants to trigger false positives or false negatives over the sliding window. One motivation for doing so is causing cache saturation or denial of service by forcing cache misses, triggering false alarms or crafting fradulent transactions without triggering fraud detection systems.

In order to create a realistic adversary model, we assume like in \cite{DBLP:journals/corr/abs-1709-08920} that the adversary does not have access to the internal memory representation of the filter. Nonetheless, after every insertion she knows whether the inserted element was detected as a duplicate or not. 

We first recall the definition of an adversarial game, adapted to our context.
\begin{definition}
	An adversary $\mathcal A$ feeds data to a sliding window DDF $\mathcal Q$, and for each inserted element, $\mathcal A$ knows whether $\mathcal Q$ answers $\DUPLICATE$ or $\UNSEEN$, but has not access to $\mathcal Q$'s internal state $\mathcal M$. The game has two distinct parts.

\begin{itemize}
	\item In the first part, $\mathcal A$ can feed up to $n$ elements to $\mathcal Q$ and learn $\mathcal Q$'s response for each insertion.
	\item In the second part, $\mathcal A$ sends a unique element $e^\star$.
\end{itemize} 
 $\mathcal A$ wins the $n$-false positive adversarial game (resp. $n$-false negative adversarial game) if and only if $e^\star$ is a false positive (resp. a false negative).
 
 \end{definition}
Variants of these games over a sliding window of size $w$ are immediate.
 
\subsection{False positive and false negative resistance}

\begin{definition}[Adversarial False Positive Resistance]
	We say that a DDF $\mathcal F$ is $(p, n)$-\emph{resistant to adversarial false positives} if no polynomial-time probabilistic (PPT) adversary $\mathcal A$ can win the $n-$false positive adversarial game with probability more than $p$.  
\end{definition}
Note that if $\mathcal F$ is $(p,n)$-resistant, then it is $(p, m)$-resistant for all $m < n$.

We define similarly the notion of being \emph{resistant to adversarial false negatives}. Finally, both definitions also make sense in a sliding window context.

\subsection{Bound on false positive resistance}

\begin{theorem}
	Let $\mathcal Q$ be a filter of $L$ subfilters $\mathcal F_i$, with $c$ insertions maximum per subfilter, let $w$ be a sliding window. 
	
	If $\mathcal F$ is $(p, c)$-resistant to adversarial false positive attacks and $cL \leq w$, then $\mathcal Q$ is $(1-(1-p)^L, w)$-resistant to adversarial false positive attacks on a sliding window of size $w$.
	
	If $cL > w$, the adversary has a probability of success of at least $1-(1-p)^L$.
\end{theorem}

\begin{proof}
	If $cL \leq w$, then information-theoretically the subfilters only have information on elements in the sliding window.
	The probability of false positive for $\mathcal Q$ is $1 - (1 - \FP_{\mathcal F, c})^L$, which is strictly increasing with $\FP_{\mathcal F,c}$. Hence, the optimal solution is reached by to maximising the probability of false positive in each subfilter $\mathcal F_i$. By hypothesis the latter is bounded above by $p$ after $c$ insertions.
	
	On the other hand, if $cL > w$ then oldest filter holds information about elements that are not in the sliding window anymore. Hence, a strategy for the attacker trying to trigger a false positive on $e^\star$ could be to make it so these oldest elements are all equal to $e^\star$. Let $E$ be the optimal adversarial stream for triggering a false positive on the sliding window $w$ with the element $e^\star$, when $cL \leq w$. The adversary $\mathcal A$ can create a new stream $E' = e^\star | e^\star | \dotsc | E$ where $e^\star$ is prepended $cL - w$ times to $E$.
	
	After $w$ insertions, the last subfilter will answer $\DUPLICATE$ with probability at least $p$, hence giving a lower bound on $\mathcal A's$ probability of success. If, for some reason, the last subfilter answers $\DUPLICATE$ with probability less than $p$, then the same reasoning as for when $cL \leq w$ still applies, hence we get the correspondig lower bound (which is, in this case, an equality).
\end{proof}

\subsection{Bounds on false negative resistance}

\begin{theorem}
	Let $\mathcal Q$ be a filter of $L$ subfilters of kind $\mathcal F$, with $c$ insertions maximum per subfilter, and let $w$ be a sliding window. 
	
	If $\mathcal F$ is $(p, c)$-resistant to adversarial false negative attacks, then $\mathcal A$ can win the adversarial game on the sliding window $w$ with probability at least $p^L$.
	
	Furthermore, for $q$ the lower bound on the probability of false positive $\FP_{\mathcal F,c}$ for a given stream, if $w \leq (L-1)c$ then $\mathcal Q$ is $(\min(1 - q, p)^{L-1}p, w)$-resistant to false negative attacks on the sliding window $w$.
	On the other hand if $w > (L-1)c$ then $\mathcal Q$ is  $(\max(1 - q, p)^{L}, w)$-resistant to false negative attacks on the sliding window $w$.
\end{theorem}

\begin{proof}
	Let us first prove that a PPT adversary $\mathcal A$ can win the game with probability at least $p^L$.
	For this, let us consider the adversarial game against the subfilter $\mathcal F$: after $c$ insertions from an aversarial stream $E_c$, $\mathcal A$ choses a duplicate $e^\star$ which will be a false negative with proability $p$.
	Hence, if $\mathcal A$ crafts, for the filter $\mathcal Q$, the following adversarial stream $E' = E_c \mid E_c \mid \cdots \mid E_c$ consisting of $L$ concatenations of the stream $E_c$, then $e^\star$ is a false negative for $\mathcal Q$ if and only if it is a false negative for all subfilters $\mathcal F_i$, hence a probability of success for $\mathcal A$ of $p^L$.
	
	Now, Let us prove the case where $w\leq (L-1)c$. In this case, at any time, $\mathcal Q$ remembers all elements from inside the sliding window. As we have seen in the previous example, the probability of success of $\mathcal A$ is strictly increasing with the probability of each subfilter to answer $\UNSEEN$. The probability of a subfilter to answer $\UNSEEN$ is:
	
	\begin{itemize}
		\item $\FN'_{\mathcal F,c}$ if $e^\star$ is in the subfilter's sub-sliding window;
		\item $1 - \FP'_{\mathcal F, c}$ if $e^\star$ is not in the subfilter's sub-sliding window
	\end{itemize}
	where $\FN'$ and $\FP'$ are the probabilities of false negative and positives on the adversarial stream (which may be different from a random uniform stream).
	
	However, since $e^\star$ is a duplicate, it is in at least one subfilter's sub-sliding window. As such, the optimal strategy for $\mathcal A$ is to maximise the probability of all subfilters to answer $\UNSEEN$.  
	Now, $\FN'_{\mathcal F,c}$ is bounded above by $p$ and $1 - \FP'_{\mathcal F, c}$ is bounded above by $1-q$, so the best strategy is where as many filters as possible answer $\UNSEEN$ with probability $\max(p, 1-q)$, knowing that at least one filter must contain $e^\star$ and as such its probability for returning $\UNSEEN$ is at most $p$, hence the result.
	
	Now, let us consider the case when $w > (L-1)c$. We have already introduce the element $e^\star$ in the last $w$ elements, and we want to insert it again. It is possible, for the adversary, to create the following stream $E = (e_1, e_2, \dotsc, e_{c-1}, e^\star, e_{c+1}, \dotsc, e_{Lc}, e_{Lc + 1})$, and to insert $e^\star$ afterwards.
	
	When $e_{Lc+1}$ is inserted, all elements $(e_1, \dotsc, e_{c-1}, e^\star)$ are dropped as the oldest subfilter is popped. Hence, in this context $e^\star$ is not in any subfilter anymore, so by adapting the previous analysis, $\mathcal A$ can get a false negative with probability at most $\max(1 - q, p)^{L}$.
\end{proof}
%% Acknowledgements 
%\begin{acks} \end{acks}

%% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{qhtv2}

%% Appendix
\appendix

\section{Effects of the simulation's finiteness}
\label{app:finite}

Theoretical results about the queuing construction apply in principe to an infinite stream. However, simulations are necessarily finite, and for very large windows (that are approximately the same size as the whole stream) this causes interesting artifacts in the error rates. Note that these effects have very little impact on practical implementations of queuing filters, since almost all use cases assume a window size much smaller than the stream (or, equivalently, a very large stream). Nevertheless we illustrate the effect of the finite simulation and the parameters affecting it, if only to motivate a further analytical study of this phenomenon.

To accentuate this effect we use the Pearson $\phi$ coefficient that measures statistical correlation (between -1 and +1) between the filter's predictions and the expected answers. A coefficient of $1$ corresponds to an always correct filter, a coefficient of $-1$ to an always incorrect one, and a coefficient of $0$ means the filter produces answers that are statistically independent from the correct ones. 

Figure~\ref{fig:strangess} measures $\phi$ as a function of $w$, for different stream sizes $N$. A visible peak in $\phi$ can be found around $w\approx N$.   

\begin{figure*}[!ht]
	\centering
	\input{graphs/stream_size.tex}
	\caption{Pearson $\phi$ for queuing SQF ($L = 10$, $M = 10^5$, $|\Gamma| = 2^{16}$) with streams of size $10^5$ to $10^8$.}\label{fig:strangess}
\end{figure*}

As can be seen on this simulation, there is only disagreement around $w \approx N/L$, and increasing $N$ results in a later and smaller peak. 

It is also possible to run simulations for different alphabet sizes $\Gamma$, which shows that the peak's position increases with $|\Gamma|$, although the relationship is not obvious to quantify.

\end{document}
