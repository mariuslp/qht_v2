\input{preamble.tex}

\begin{document}

\title{Adapting Duplicate Filters to a Sliding Window Context}

%% Replace by \iffalse when submitting anonymously 
\iftrue 
	\author{Rémi Géraud-Stewart}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \state{France}
	} \email{remi.geraud@ens.fr}

	\author{Marius Lombard-Platet}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \country{France}
	}
	\affiliation{%
	\institution{Be-Studys}
	\city{Geneva} \country{Switzerland}
	} \email{marius.lombard-platet@ens.fr}

	\author{David Naccache}
	\affiliation{%
	\institution{Département d'informatique de l'ENS, ENS, CNRS, PSL Research University}
	\city{Paris} \country{France}
	} \email{david.naccache@ens.fr}
\fi 

%% Abstract
\begin{abstract}
	A duplicate filter is a data structure designed to efficiently detect repetitions (duplicates) in a stream, when only a limited amount of memory is available. A well-known data structure used in this contexte is the Bloom filter, which has been generalised in several ways to 
	improve its performances or strike specific tradeoffs. 
	
	When analysed in the limit of an infinite stream, error rates of duplicate filters turn out to be heavily constrained; as a consequence they appear to provide no advantage, asymptotically, over a random yes-no coin toss \cite{10.1145/3297280.3297335}.

	In this paper we introduce a generic framework that adapts duplicate filters to the better-behaved context of a sliding window. We show that several duplicate filters can be adapted to this context and provide a rigourous mathematical study of such constructions.
\end{abstract}

%% CCS XML
\begin{CCSXML}
	% Get code at http://dl.acm.org/ccs.cfm.
	% Please copy and paste the code
\end{CCSXML}

%% Keywords
\keywords{
	% TODO
}

\maketitle

\section{Introduction}
\subsection{Motivation}

\paragraph{Duplicate detection problem.}
We consider the following problem (duplicate detection problem, DDP): given a stream $E_n = \{e_1, e_2, \dotsc, e_n\}$ and a \enquote{new} item $e^\star$, find whether $e^\star \in E_n$, using only $M$ bits of memory. It is understood that the stream $E_n$ is updated at every time increment. An \emph{efficient} solution to this problem provides an answer with bounded error.

Instances of this problem abound in computer science, with applications in file system indexation, database queries, network load balancing, network management \cite{10.5555/647912.740658}, in credit card fraud detection \cite{DBLP:journals/corr/abs-1709-08920}, phone calls data mining \cite{10.1145/347090.347094}, etc. A discussion about algorithms on large data streams can be found in \cite{10.1145/776985.776986}. 
Even when the stream is bounded, i.e. $\lim_{n\to\infty}|E_n| < \infty$, it may be impractical to perform a complete lookup and approximate solutions are desirable. 

\paragraph{Fuzzy and approximate DPP}
A popular approach for the DDP is \emph{fuzzy} detection \cite{10.5555/1287369.1287420,1410199,10.1109/ICDE.2011.5767865,10.1109/ICDE.2012.20,Monge97anefficient}. Fuzzy detection deals with detecting \emph{fuzzy} duplicates (i.e., detecting if two elements are similar), which can be useful in several context, such as text processing.

This is however out of the scope of our work, as we consider an element to be a duplicate if and only if the exact same element has already appeared in the stream. \emph{Approximate} detection states that no attempt will be done to reach perfect accuracy, but rather to limit as much as possible the errors. As a matter of fact, in the context of approximate DDT, it is assumed that $M$ is to small for classical naïve solutions. One can prove \cite[Theorem 2.1]{10.1145/3297280.3297335} that perfect detection of duplicates in a stream with $U$ possible values requires $U$ bits of memory: even for modest situations (e.g. 64-bit nonces, corresponding to $2^{64}$ possible values) this is quickly impractical.

Approximate duplicate detection has many real-life use cases, and can sometimes play a critical role, for instance in cryptographic schemes where all security and secrecy fall apart as soon as a random nonce is used twice, such as the ElGamal \cite{10.1007/3-540-39568-7_2} or ECDSA signatures. Other uses include improvements over caches \cite{4484874}, duplicate clicks \cite{10.1145/1060745.1060753}, and others.

\todo[continuer].

\subsection{Contributions}

\subsection{Related Work}\label{sub:related}
\todo[continuer]
In \cite{10.1145/1060745.1060753}, the authors explore the behaviour of a structure very close to the counting Bloom filter in some specific sliding window context, that they call landmark- and jumping sliding windows. However, they do not deal with the problematic of a sliding window where at each new element added to the sliding window, another element is removed.
Closer to our work, the idea of using subfilters, tossing old ones has already been explored in the A2 filter \cite{Yoo10}. However, the author only considered a small portion of the problem, as they only discussed about using two subfilters, and only Bloom Filters acting as subfilters.

In this work, we provide a full abstraction of the mechanism, allowing us to use many other DDF than Bloom Filters. We also provide a generalised analysis of the error rates.
\subsection{Organisation}


\section{Duplicate Detection and Filter Saturation}
\subsection{Notations and Initial Problem Statement}
Let us consider an alphabet $\Gamma$. A stream $E$ over $\Gamma$ is a (possibly infinite) set $E = \{e_1, e_2, \dotsc\}$, with each $e_i \in \Gamma$.

\begin{definition}[Duplicate and unseen element]
Let $E = \{e_1, e_2, \dotsc\}$ be a stream over $\Gamma$. Some element $e_j \in E$ is called a \emph{duplicate} if and only if there exists $i < j$ such that $e_i = e_j$. In that case we write $e_j \dup E$. Otherwise, $e_j$ is said to be \emph{unseen} and we write $e_j \notdup E$.
\end{definition}

\begin{definition}[Duplicate detection filter]
	A \emph{duplicate detection filter} (DDF) is a collection 
	$\mathcal F = (\mathcal S, \mathsf{Insert}, \mathsf{Detect})$ of a state\footnote{Here, $\mathcal M$ denotes all the memory states that can be reached by the filter.} $\mathcal S \in \mathcal M$ and two algorithms,
	\begin{itemize}
		\item $\mathsf{Detect}: \mathcal M \times \Gamma \to \{\DUPLICATE, \UNSEEN\}$;
		\item $\mathsf{Insert}: \mathcal M \times \Gamma \to \mathcal M$.
	\end{itemize}
	Informally, the $\mathsf{Detect}$ algorithm tentatively labels an element $e$ as duplicate or not, given the current state $\mathcal S$; whereas $\mathsf{Insert}$ takes as input the current state, an element $e$ to insert in the filter, and returns an updated memory state $\mathcal S'$. 
\end{definition}

We will usually consider the situations where the available memory is too small for perfect detection, i.e., $|\mathcal S| \ll |\Gamma|$.

\begin{definition}[False positive, false negative]
	Let $e \dup E$ (resp. $e \notdup E$) and $\mathcal F$ be a filter. We say that $e$ is a \emph{false positive} (resp. \emph{false negative}) when $\mathcal F.\mathsf{Detect}(e) = \DUPLICATE$ (resp. $\UNSEEN$).
\end{definition}

\begin{definition}[False positive rate, false negative rate]
	Let $E$ be a stream, $\mathcal F$ be a filter. The \emph{false positive rate} (resp. \emph{false negative rate}) of $\mathcal F$ over the first $i < n$ elements, denoted $\FPR_i$ (resp. $\FNR_i$), is defined as the ratio of false positives divided over the number of unseen elements in $\{e_1, \dotsc, e_i\}\subset E$ (resp. the ratio of false negative divided by the number of duplicate elements in that subset).
\end{definition}

However, it is more practical to work with another value, namely, the probability that after $m$ insertions, the element $e^\star$ is a false positive (resp. false negative). 

\begin{definition}[Probability of false positive, of false negative]
	Let $E = \{e_1, \dotsc\}$ be a stream, $\mathcal F$ be a filter. The \emph{probability of false positive} (resp. \emph{probability of false negative}) of $\mathcal F$ after $i$ elements, denoted $\FP_i$ (resp. $\FN_i$), is defined as the probability that $e_{i+1}$ is a false positive (resp. false negative).
\end{definition}

One can observe that $\FPR_n = \frac{1}{n}\sum_{i=1}^m \FP_i$, and similarly for $\FNR_n$.

\subsection{Filter Saturation and Sliding Windows}
Unfortunately, efficient duplicate detection over an infinite stream has been identified as an unsolvable problem \cite{10.1145/3297280.3297335}. As a matter of fact, as the number of bits of information of the stream grow to infinity, the filter can only remember an ever diminishing proportion of those bits, eventually having no advantage compared to a filter answering randomly.

In other terms, any DDF is asymptotically as good as a random guess:
	\begin{theorem}[\cite{10.1145/3297280.3297335}]\label{thm:sat_u}
	Let $\FNR_\infty$ and $\FPR_\infty$ be the asymptotic false negative and false positive rates of a filter of $M$ bits of memory (i.e. the FNR and FPR on a stream of size going to infinity).
	
	If $|\Gamma| \gg M$, then $\FNR_\infty + \FPR_\infty = 1$, which characterises random filters (i.e. filters answering randomly to any query).
\end{theorem}

One way out of this situation is to lower our expectations and work on a sliding window:

\begin{definition}[Sliding window DDP]
	Given a stream $E_n$ of fixed size $w$, with $E_n = \{e_{n-w+1}, \dotsc, e_n\}$ and an item $e^\star$, find whether $e^\star \in E_n$.
\end{definition} 

We therefore introduce new definitions suited to that setting:

\begin{definition}[Duplicate element, unseen element over a sliding window]
	Let $E = \{e_1, e_2, \dotsc\}$ be a stream over $\Gamma$, let $w\in \mathbb N$, an element $e_j \in E$ is called a \emph{duplicate} in $E$ over the sliding window of size $w$ if and only if there exists $j$ such that $j - w \leq i < j$  and $e_i = e_j$. In this case we write $e_j \wdup E$. Otherwise, $e_j$ is said to be \emph{unseen} and we write $e_j \notwdup E$.
\end{definition}

\begin{definition}[False positive, false negative over a sliding window]
	Let $w \in \mathbb N$, $e \notwdup E$ (resp. $e \wdup E$) and let $\mathcal F$ be a DDF. We say that $e$ is a \emph{false positive} (resp. \emph{false negative}) over $w$ if $\mathsf{Detect}(e) = \DUPLICATE$ (resp. $\UNSEEN$).
\end{definition}

\begin{definition}[False positive rate, false negative rate over a sliding window]
	Let $E$ be a stream, $\mathcal F$ be a DDF, $w \in \mathbb N$. The \emph{false positive} (resp. \emph{false negative}) rate of $\mathcal F$ over the first $i < n$ elements and the sliding window $w$, noted $\FPR^w_i$ (resp. $\FNR^w_i$), is defined as the number of false positives over $w$ divided by the number of unseen elements in $E$ over $w$ (resp. the number of false negative over $w$ divided by the number of duplicate elements over $w$).
\end{definition}


\subsection{Bounds on the Sliding Window DDP}

Our goal is to minimise both the false positive and false negative rates of a DDF, given a fixed and limited amount of memory. Before going into details, we first explore the limits of the problem, i.e., when perfect or very performant solutions exist.

\begin{theorem}
	For $M = w (\log_2(w) + 1) \log_2(|\Gamma|)$, the sliding window DDP can be solved with 0 errors in constant time.
\end{theorem}

\begin{proof}
	To store $w$ elements from a sliding window, the naive idea is to use a queue $Q$ of $w$ elements. However, this yields a complexity of $O(w)$ for lookup, which might be impractical. Thus, we create a second structure for the sake of quickly answering lookup questions.
	
	The dictionary $D$ has the form $\Gamma \Rightarrow \mathbb N$: keys are elements from the alphabet, values are integers serving as counters.
	
	During insertion of $e$, $e$ is appended to the queue and $D[e]$ is incremented. If the key $e$ did not exist in $D$, it is created first.
	
	The insertion of an element means that another element, at the end of the sliding window, is removed. For this, we pop the last element of $Q$, and get $e_{last} \gets Q.pop()$. Then, we decrement $D[e_{last}]$ and if $D[e_{last}] = 0$, the key is deleted from $D$.
	
	Lookup of an element $e^\star$ is simply done by looking whether the key $D[e^\star]$ exists, which is done in constant time.
	
	It is immediate to see that such a structure has a 0 error rate over the sliding window and that both insertion and lookup times are constant.
	
	The size of the queue is $w\log_2 |\Gamma|$, the size of the dictionary is $w \log_2 |\Gamma| \log_2 w$ (as the dictionary cannot have more than $w$ keys as the same time, and a counter cannot go over $w$, thus being less than $\log_2 w$ bits long), hence the claim. 
\end{proof}


However, this implementation depends on $\log_2 |\Gamma|$, which might be arbitrarily large. We thus propose another solution, with a very low error rate but constant time.

\begin{theorem}
	Let $w \in \mathbb N$. Let $M \simeq 2w\log_2w$, then the sliding window DDP can be solved with almost no error using $M$ bits of memory.
	
	More precisely, it is possible to create a filter of $M$ bits with an FNR of $0$, and an FPR of $1 - (1-\frac1{w^2})^w \sim \frac 1w$.
\end{theorem}

\begin{proof}
	Let $h$ be a hash function with codomain $\{0,1\}^{2 \log_2 w}$.
The birthday theorem, as summed up in \cite{10.1007/3-540-45708-9_19}, states that for a hash function $h$ over $a$ bits, one must on average collect $2^{a/2}$ input-output pairs before obtaining a collision. Therefore $2^{(2 \log_2 w) / 2} = w$ hash values $h(e_i)$ can be computed before having a $50\%$ probability of a collision (here, a collision is when two distinct elements of the stream $e_i, e_j$ with $i \neq j, e_i \neq e_j$ have the same hash, i.e. $h(e_i) = h(e_j)$). The 50\% threshold we impose on $h$ is arbitrary but nonetheless practical.

Let $\mathcal F$ be the following DDF: the filter's state consists in a queue of $w$ hashes, and for each new element $e$, $\mathsf{Detect}(e)$ returns \DUPLICATE if $h(e)$ is present in the queue, \UNSEEN otherwise. $\mathsf{Insert}(e)$ appends $h(e)$ to the queue before popping the list. 

There is no false negative, and a false positive only happens if the new element to be inserted collides with at least one other element, which happens with probability $1 - (1 - \frac 1{2^{2\log_2w}})^{w} = 1 - (1-\frac1{w^2})^w$, hence an FNR of $0$ and a FPR of $1 - (1-\frac1{w^2})^w$.
The queue stores $w$ hashes, and as such requires $w \cdot 2\log_2 w$ bits of memory.
\end{proof}

When $\log_2|\Gamma| > 2 \log_2 w$ (which is the case of most practical use cases) this DDF outperforms the naïve strategy\footnote{The naïve strategy consisting of storing the $w$ elements of the sliding window, requiring $w \log_2|\Gamma|$ bits of memory.}, both in terms of time and memory.


Note that this solution has a time complexity of $O(w)$. Using a dictionary like in the previous proof, we get a filter with an error rate of about $\frac 1w$ and constant time for insertion and lookup, but requiring $5w\log_2 w$ bits of memory.

Hence, very good solutions for situations where $M \approx 5w\log_2 w$ already exist, but efficient solutions and lower bounds on the error rates for smaller amounts of memory are still to be found.


\subsection{DDP vs Sliding Window DDP}
The sliding window variant of DDP may seem to be a slight weakening of the original problem. Remarkably, it turns out that DDFs which perform well for the DDP do rather poorly for the sliding window DDP. A literature review collects the following DDF constructions: A2 filters \cite{Yoo10}, Stable Bloom Filters (SBF) \cite{Den06}, Quotient Hash Tables (QHT) \cite{10.1145/3297280.3297335}, Streaming Quotient Filters (SQF)\footnote{QHTs stem from a correction of SQFs, and as such SQFs are less efficient than QHT in every situation \cite{10.1145/3297280.3297335}. We included them for the sake of completeness.} \cite{Dut13}, Block-decaying Bloom Filters (b\_DBF) \cite{She08}, and a slight variation of Cuckoo Filters \cite{Fan14} suggested by \cite{10.1145/3297280.3297335}. The structure proposed in \cite{10.1145/1060745.1060753} is not suited to either sliding window or no sliding window DDP, as they work on what they call `landmark` sliding window, which is a zero-resetting of the memory at some user-defined epochs.

Of these, only the A2 and b\_DBF were designed for the sliding window DDP. The others do not account for a finite-length sliding window, and therefore cannot be adjusted as a function of $w$. Another remarkable (but experimental) observation is that these DDFs' false positive rate does not decrease to 0 when the sliding window becomes small.

\todo[Expliquer ?]


\section{Queuing Filters for Sliding Window Adaptation}
In this section, we describe how to turn DDF into sliding window DDFs. We first give the description of the setup, before studying the theoretical error rates.

\subsection{Queueing Filter}
Let $\mathcal F$ be a DDF, not designed to work on a sliding window. Our idea, instead of spawning a filter $\mathcal F$ using as much memory as possible, is to create $L$ small versions of $\mathcal F$, which all have a limited timespan and can insert up to $c$ elements in their structure. We call these small filters subfilters. The subfilters are organised in a queue.

When inserting a new element in the queueing filter, it is inserted in the topmost subfilter of the queue. After $c$ insertions, a new empty filter is added to the queue, and the oldest subfilter is popped and erased.

As such, we can consider that each subfilter operates on a sub-sliding window of size $c$. Given that we have $L$ subfilters and we want to operate on a sliding window of size $w$, we require $cL \approx w$.

A scheme describing our structure is detailed in \ref{fig:scheme}.

\begin{figure}
	\begin{tikzpicture}[scale=0.4]
	
	% F_0 under construction
	\draw[pattern=north west lines, pattern color=gray, draw=none] (1, 0) rectangle (2, 1);
	\draw (0, 0) rectangle (2, 1) node[midway] {$\mathcal F_0$};

	
	% Other subfilters
	\draw[pattern=north west lines, pattern color=gray] (3, 0) rectangle (5, 1) node[midway] {$\mathcal F_1$};
	\draw[pattern=north west lines, pattern color=gray] (6, 0) rectangle (8, 1) node[midway] {$\mathcal F_2$};
	\node at (9.5, 0.5) {...};
	\draw[pattern=north west lines, pattern color=gray] (11, 0) rectangle (13, 1) node[midway] {$\mathcal F_{L-1}$};
	
	% old subfilters
	\draw[dashed] (14, 0) rectangle (16, 1) node[midway] {$\mathcal F_{L}$};
	\draw[dashed] (17, 0) rectangle (19, 1) node[midway] {$\mathcal F_{L+1}$};
	
	% arrows
	\draw [<->,>=latex] (6, 1.5) -- (8, 1.5) node[above, midway] {$c$ elements};
	\draw [<->,>=latex] (0, -3) -- (13, -3) node[below, midway] {Capacity of $Lc \approx w$ elements};
	\draw [<->,>=latex] (1, 3) -- (15, 3) node[above, midway] {Current sliding window (of size $w$)};
	
	%stream 
	\draw [<-, >=latex] (0, 5.5) -- (19, 5.5) node[above, midway] {Stream};
	\foreach \x in {1, 3,...,18}
	\draw (\x, 5.7) -- node[pos=0.5] (point\x) {} (\x, 5.3);
	
	\foreach \x in {2, 4,...,18}
	\draw (\x, 5.6) -- node[pos=0.5] (point\x) {} (\x, 5.4);

	\path (point1) node [below] {$e_m$};
	\foreach \x [evaluate=\x as \i using int(\x-1)] in {3, 5,..., 9}
	\path (point\x) node [below] {$e_{m-\i}$};
	\path (point11) node [below] {$\dots$};
	\path (point15) node [below] {$e_{m - w + 1}$};
	
	%queueing filter
	\draw (-0.5, 2.5) rectangle (13.5, -2.5) node[anchor=south east] {Queuing filter};
	
	% legend
	\draw[draw=none, pattern=north west lines, pattern color=gray] (0.5, -5) rectangle (1, -5.5);
	\draw (0, -5) rectangle (1, -5.5) node[midway] {\tiny $\mathcal F$};
	\draw (1.25, -5.25) node[anchor=west] {\tiny Subfilter being populated};
	
	\draw[pattern=north west lines, pattern color=gray] (7, -5) rectangle (8, -5.5) node[midway] {\tiny $\mathcal F_i$};
	\draw (8.25, -5.25) node[anchor=west] {\tiny Populated active subfilter};
	
	\draw[dashed] (14, -5) rectangle (15, -5.5) node[midway] {\tiny $\mathcal F_j$};
	\draw (1 5.25, -5.25) node[anchor=west] {\tiny Expired subfilter};
	\end{tikzpicture}
	\caption{Architecture of the queuing filter. The filter is composed of $L$ subfilters $\mathcal F$, each containing up to $c$ elements. Once the newest subfilter has inserted $c$ elements in its structure, the oldest one is expired. As such, it is dropped and a new one is created and put under population at the beginning of the queue. In this example, the sub-sliding window of $\mathcal F_1$ is $\{e_{m-2}, e_{m-3}, e_{m-4}\}$.} \label{fig:scheme}
\end{figure}

When the user wants to lookup for an element, they query all subfilters. If any subfilter answers with \DUPLICATE, the queueing filter answers \DUPLICATE. Otherwise, it answers \UNSEEN. Insertion is a simple insertion in the topmost subfilter.

After $c$ insertions (where $c$ is a filter parameter), the last filter of the queue is removed, and a new (empty) filter is appended in front of the queue. We give a brief pseudo-code of the queuing filter's functions \textsf{Lookup} and \textsf{Insert}, as well as a \textsf{Setup} function for initialisation, in \ref{alg:queueing}.

For the sake of simplicity in the pseudocode, we assume that the subfilter $\mathcal F$ has a function \textsf{Setup} which takes as input the available memory $m$, and returns an initialized empty filter $\mathcal F$ of size at most $m$..

\begin{algorithm}[]
	\caption{Queueing Filter Setup, Lookup and Insert}\label{alg:queueing}
	
	\begin{algorithmic}[1]
	\Function{Setup}{$\mathcal F, M, L, c$}
		\Comment $M$ is the available memory, $\mathcal F$ the subfilter structure, $L$ the number of subfilters and $c$ the number of insertions per subfilter
		\State Queue\_storage $\gets$ empty queue
		\State Counter $\gets 0$
		\For{$i$ from $0$ to $L-1$}
			\State Queue\_storage[i] $\gets$ $\mathcal F.\mathsf{Setup}(\lfloor M/L \rfloor)$
		\EndFor
		Counter $\gets$ 0
		\State Store $c$, $M$, $L$, $\mathcal F$
		
		\State \Return filter
	\EndFunction 
	\item[]
	\end{algorithmic}

	\begin{algorithmic}[1]
	\Function{Lookup}{$e$}
		\For{$i$ from $0$ to $L-1$}
			\If{$\mathcal F_i.\mathsf{Lookup(e)}$}
				\State \Return $\DUPLICATE$
			\EndIf
		\EndFor
		\State \Return \UNSEEN
	\EndFunction
	\item[]
	\end{algorithmic}

	\begin{algorithmic}[1]
	\Function{Insert}{$e$}
		\State $\mathcal F_0.\mathsf{Insert}(e)$
		\State Counter++
		\If{Counter == $c$}
			\State Pop last element of Queue\_Storage
			\State Insert $\mathcal F$.\textsf{Setup}($\lfloor M/L \rfloor$) at the beginning of Queue\_Storage
		\EndIf
	\EndFunction
	\end{algorithmic}
\end{algorithm}

\subsection{Error Rate Analysis}
Depending on the underlying subfilter, the queueing filter will behave differently. However, its false positive and false negative probabilities can be derived from the underlying subfilter's false positive and false negative probabilities. 

\subsubsection{Number of Elements in a Queueing Filter}\label{subsub:number_elements}
Before deriving the error rates, it is important to analyse how many elements are in the filter. We know, by design, that no more than $cL$ elements can be present. However, after exactly $cL$ insertions, the last subfilter is deleted, to be replaced by an empty one at the beginning of the queue. Hence, after $cL$ insertions, only $c(L-1)$ elements are stored in the queueing filter.

Hence, the number of elements in the filter oscillate between $c(L-1)$ and $cL$. Furthermore, we have $cL \approx w$, which means that the number of elements in the filter can be higher than $w$. These facts must be kept in mind while deriving the error rates.

\subsubsection{Probability of False Positive}
\begin{theorem}
	Let $\mathcal Q$ be a queueing filter, consisting of $L$ underlying subfilters $\mathcal F$, each filled with up to $c$ elements. Let $\FP_{\mathcal F,m}$ be the probability of false positive of the subfilter $\mathcal F$ after $m$ insertions, let $\FP_{\mathcal Q, m}^w$ be the probability of false positive of $\mathcal Q$ after $m$ insertions on a sliding window of size $w$.
	
	We have $$\FP_{\mathcal Q, m}^w = 1 - (1 - \FP_{\mathcal F,c})^{L-1}(1 - \FP_{\mathcal F, m \% c })$$ where \% is the modulo operator.
\end{theorem}

\begin{proof}
	Let $E = \{e_1, \dotsc, e_m, \dotsc\}$ be a stream, let $w$ be a sliding window and let $e^\star \notwdup E$. We first take into account the remark in \ref{subsub:number_elements} that the filter $\mathcal Q$ may contain more than $w$ elements, and hence potentially contain $e$ in memory, in the last subfilter of the queue. However, such event is quite rare, and can be neglected: let $\delta = cL - w$ the number of additional elements that can be stored in $\mathcal Q$. The probability that $e^\star$ has occurred in $\{e_{m-cL}, \dotsc e_{m-w+1}\}$ is $1-\left(1-\frac1{|\Gamma|}\right)^\delta \approx \frac{\delta}{|\Gamma|} \approx 0$ since $\delta$ is likely to be small, while $|\Gamma|$ is too big for being stored in memory.
	
	Hence, we can neglect the probability that $e^\star$ is present in the filter. Therefore, $e^\star$ is a false positive if and only if at least one subquery $\mathcal F_i.\mathsf{Lookup(e^\star)}$ returns \DUPLICATE. Conversely, $e^\star$ is not a false positive if and only if all subqueries $\mathcal F_i.\mathsf{Lookup(e^\star)}$ return \UNSEEN. This can be rewritten as $e^\star$ is not a false positive if and only if $e^\star$ is not a false positive for each subfilter.
	
	Given that each subfilter stores $c$ elements, safe for the first subfilter which only stores $m \% c$ elements, we get
	$$\FP_{\mathcal Q, m}^w = 1 - (1 - \FP_{\mathcal F, m \% c })(1 - \FP_{\mathcal F,c})^{L-1}$$
\end{proof}

\subsubsection{Probability of False Negative}\label{sub:fnr}
\begin{theorem}
	Let $\mathcal Q$ be a queueing filter, consisting of $L$ underlying subfilters $\mathcal F$, each filled with up to $c$ elements. Let $\FN_{\mathcal F,m}$ be the probability of false negative of the subfilter $\mathcal F$ after $m$ insertions, let $\FN_{\mathcal Q, m}^w$ be the probability of false negative of $\mathcal Q$ after $m$ insertions on a sliding window of size $w$.
	
	For $p_m = \frac{1-\left(1-\frac 1{|\Gamma|}\right)^m}{1-\left(1-\frac 1{|\Gamma|}\right)^w}$, we have 
	\begin{align*}
	\FN_{\mathcal Q, m}^w =& \left[p_c\FN_{\mathcal F,c} + \left(1-p_c\right)\left(1-\FP_{\mathcal F,c}\right)\right]^{L-1}
	\\&\qquad
	 \cdot \left[p_{m\%c}\FN_{\mathcal F,m\%c} + \left(1-p_{m\%c}\right)\left(1-\FP_{\mathcal F,m\%c}\right)\right]
	\end{align*}
	
	where $\%$ is the modulo operator.
\end{theorem}

\begin{proof}
	Let $E = \{e_1, \dotsc, e_m, \dotsc\}$ be a stream, let $w$ be a sliding window and let $e^\star \wdup E$.
	We first observe that if $cL > w$, there might be some duplicates of $e^\star$ that happened before the current sliding window, but might still be remembered by the oldest filter. However, as argued in the previous section, the probability of such an event is negligible.
	
	Hence, we consider that the only duplicates are part of the sliding window. $e^\star$ is a false negative if and only if all subfilters $\mathcal F_i$ answer $\mathcal F_i.\mathsf{Detect}(e^\star) = \UNSEEN$.
	
	There can be two cases:
	\begin{itemize}
		\item $e^\star$ is present in $\mathcal F_i$'s sub-sliding window
		\item $e^\star$ is not present in $\mathcal F_i$'s sub-sliding window
	\end{itemize}

In the first case, $\mathcal F_i.\mathsf{Detect}(e^\star)$ returns \UNSEEN if and only if $e^\star$ is a false negative for $\mathcal F_i$. Hence, in this case, the probability of $\mathcal F_i$ to return \UNSEEN is $\FN_{\mathcal F,c}$. For the subfilter $\mathcal F_0$, however, the probability is $\FN_{\mathcal F, m\%c}$.

In the second case, $\mathcal F_i.\mathsf{Detect}(e^\star)$ returns \UNSEEN if and only if $e^\star$ is not a false positive for $\mathcal F_i$. Hence, in this case, the probability of $\mathcal F_i$ to return \UNSEEN is $1-\FP_{\mathcal F,c}$. For the subfilter $\mathcal F_0$, however, the probability is $1-\FP_{\mathcal F, m\%c}$.

Now, the probability $p_c$ for $e^\star$ to be in subfilter $\mathcal F_i$ (which has had $c$ insertions) sub-sliding window is 

\begin{align*}
	p_c =& Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window | } e^\star \wdup E]\\
	=& \frac{Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window } \cap e^\star \wdup E]}{Pr[e^\star \wdup E]}\\
	=& \frac{Pr[\text{$e^\star$ is in }\mathcal F_i \text{ sub-sliding window}]}{Pr[e^\star \wdup E]}\\
	=& \frac{1 - Pr[\text{$e^\star$ is not in }\mathcal F_i \text{ sub-sliding window }]}{1 - Pr[e^\star \notwdup E]}\\
	p_c=& \frac{1 - \left(1 - \frac 1{|\Gamma|}\right)^c}{1 - \left(1-\frac{1}{|\Gamma|}\right)^w}
\end{align*}

Finally, we get that the probability of a false positive for $\mathcal Q$ over the sliding window $w$ is 
\begin{align*}
\FN^w_{\mathcal Q, m} =& \left[p_c\FN_{\mathcal F,c} + (1-p_c)(1-\FP_{\mathcal F,c})\right]^{L-1}
\\&\quad
\cdot \left[p_{m\%c}\FN_{\mathcal F,m\%c} + (1-p_{m\%c})(1-\FP_{\mathcal F,m\%c})\right]\\
\end{align*}
\end{proof}

This probability is quite complicated and does not allow, in the general case, for an in-depth analysis of the probability of false negative.

\begin{theorem}
	In first approximation, a low value of $L$ leads to a higher false negative probability for the queueing filter.
\end{theorem}

\begin{proof}
As a matter of fact, on average the queueing filter consists of $L-1$ subfilters with $c$ insertions, and one subfilter with $\frac c2$ insertions. As such, we can say that the queueing filter is a representation of the last $(c-\frac12) L$ last elements. 

Let us consider a duplicate $e^\star \wdup E$. If, in the sliding window, all duplicates only happened at least $(c-\frac12)L$ epochs ago (which happens with probability $\left(1-\frac1{|\Gamma|}\right)^{(c-\frac12)L}$), we know that no duplicates happened in any subfilter's sliding window. By using the same reasoning as the theorem's proof, we get a probability of false negative of around $\left[0 + \left(1-\frac1{|\Gamma|}\right)^{(c-\frac12)L}(1-\FP_{\mathcal F,c})\right]^L$. Given that $cL \approx w$ which is a constant, we get that the probability of false positive mostly varies, in this case, with $\left(1-\FP_{\mathcal F,c}\right)^L$. 

Hence, the lower $L$ is, the higher the probability of false negative will be.

The approximations made in this proof are backed by experimental results.
\end{proof}


\section{Application on Existing DDFs}
As our queuing filter relies on another DDF subfilter, we must pick one existing DDF from the literature.
\subsection{DDF Subfilter Selection Strategy}
As a matter of fact, it is preferable to use only one kind of DDF as subfilter, as proven by the following theorem.

\begin{theorem}
	An optimal queueing filter only uses one kind of subfilter.
\end{theorem}

\begin{proof}
	One could imagine a queueing filter in which subfilters would alternate between, say, subfilter of type $A$ and of type $B$. However, we show with a simple symmetry argument that this kind of queueing filter is sub-optimal.
	
	Let us consider a queueing filter relying on several kinds of subfilters. Given that all subfilters play the same role, and that their efficiency is independent of the other subfilters present, we know that one less efficient subfilter can be replaced by a more efficient one, directly improving the efficiency of the queueing filter.
\end{proof}

However, making a theoretical analysis of all DDFs error rates may be tedious. Instead, we propose a ranking on existing DDFs. As \cite{10.1145/3297280.3297335} mentioned, filters all reach a saturation state. The idea is then to order DDFs by their resistance to saturation --i.e., by ranking them by the time they take to reach saturation.

Before ranking the DDFs by the error rate, we first introduce a lower bound on said rate. 

Note: given we are working on selecting the best subfilter for our queueing filter, in this section we are \emph{not} working over a sliding window.

\subsection{Optimal Error Probability on a DDF}\label{sub:optimal_ddf}
    \begin{theorem}
	Let $E$ be a stream of $n$ elements uniformly selected from an alphabet of size $|\Gamma|$. For any DDF of size $M$, the error probability $ER_n$ (consisting of the sum of the FP and FN probabilities) is such that
	$$ER_n \geq 1 - \frac{1 - \left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}$$ for any $n > M$.
	
	Especially, for any filter of size $M$, the asymptotic error rate $ER_\infty$ is such that $ER_\infty \geq \left(1 - \frac{1}{|\Gamma|}\right)^{M}$.
\end{theorem}
We remind that in this section, we are working on DDP over the whole stream and not over a sliding window.

\begin{proof}
	According to classical results on information theory, a perfect filter must use at least $1$ bit to remember one element from a random uniform source with no possible error.
	
	Since the filter has $M$ bits of memory, we conclude that a perfect filter can store at most $M$ elements in memory. Assume that such a filter exists. Because the stream is random, we can assume without loss of generality that the filter stores the M last elements of the stream: any other strategy cannot yield a lower error rate, given the stream is random.
	
	Let us derive the error rate of such a filter.
	If an element is already stored in the filter, then the optimal filter will necessarily answer \DUPLICATE. On the other hand, if the element is not in memory, a perfect filter can choose to answer randomly. Let $p$ be the probability that a filter answers \DUPLICATE when an element is not in memory. An optimal filter will lower the error rate of any filter using the same strategy with a different probability.
	
	An unseen element, by definition, will be unseen in the $M$ last elements of the stream, and hence will not be in the filter's memory, so the filter will return \UNSEEN with probability $1-p$. For this reason, this filter has an FP probability of $p$.
	
	On the other hand, a duplicate element $e^\star \dup E$ is classified as \UNSEEN if and only if it was not seen in the last $M$ elements of the stream, and the filter answers \UNSEEN. Let $D$ be the event "\textsf{There is at least one duplicate in the stream} and $C$ be the event \textsf{There is a duplicate of $e^\star$ in the $M$ previous elements of the stream}.
	
	We have that $e^\star$ triggers a false negative with probability
	
	\begin{align*}
	\FN_n &= (1 - P[C | D])(1-p) =  \left(1 - \frac{P[C \cap D]}{P[D]}\right)(1-p)\\
	& = \left(1 - \frac{P[C]}{P[D]}\right)(1-p) =  \left(1 - \frac{1 - P[\bar C]}{1 - P[\bar D]}\right)(1-p) \\
	\FN_n &=  \left(1 - \frac{1 - \left(1 - \frac 1{|\Gamma|}\right)^M}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}\right)(1-p)
	\end{align*}
	
	Hence, the error probability of the filter is $ER_n = \FN_n + p =  \left(1 - \frac{1 - \left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}\right)(1-p) + p =$ $ 1 - \frac{1-\left(1 - \frac{1}{|\Gamma|}\right)^{M}}{1 - \left(1 - \frac 1{|\Gamma|}\right)^n}(1-p)$, which is minimized when $p = 0$.
	
	Given that, by definition, a perfect filter has the lowest error rate of any filter, we get the desired result.
	\end{proof}

Note, as highlighted in the proof, that this bound is not tight, and that better bounds may exist. Also note that this bound only applies when the stream is random: when patterns are found in the stream, information theory informs that data may be stored more efficiently.

As such, the best filter will be the one the closest to this error rate.

\subsection{DDF Comparison}
A filter comparison was already made in \cite{10.1145/3297280.3297335}. Taking the same filters parameters, created another benchmark, studying the resistance of each DDF to saturation. We added the lower bound on the error rate. More details about the experiments are given in \ref{sub:saturation}. We observe in \ref{fig:graph_n} that the two most efficient filters are the QHT \cite{10.1145/3297280.3297335} and the A2 \cite{Yoo10}.

However, as noted in \ref{sub:related}, A2 filters can be considered as a queuing filter, with $L=2$ and Bloom filters as subfilters. Hence, as it is not interesting to put a queueing filter inside a queueing filter, we will rather use QHT for practical benchmarking of the queueing filter.

\subsection{Parameter Tuning}
The parameters of a queueing filter are: the parameters of the subfilter, the number $L$ of subfilters and the number $c$ of insertions per subfilter. 

However, the goal here is not to modify the parameters of the underlying subfilter, but rather to tune the queueing filter so the subfilter can perform as well as possible. Hence, the queueing filter's parameters are only $c$ and $L$.

Furthermore, we have the relation $cL \approx w$, so there is only one free parameter. 

As such, an optimal parameter tuning would be to set $L$ so that the error rate (defined as the sum of the FPR and FNR) would be minimal. Nonetheless, given the complicated nature of the FPR and FNR formulae, deriving an optimal $L$ is out of reach.

We have observed in \ref{sub:fnr} that the FN is positively correlated with
$\left(1-\FP_{\mathcal F,c}\right)^L$, however the FP is negatively correlated with the same value. As such, decreasing the probability of false positive leads to an increase in the probability of false negatives. 

However, practical tests show (see \ref{sub:L_bench}) that setting $L = \sqrt w$ leads to optimal results.

Hence, the (heuristic) optimal solution is to set $L = \sqrt w$.


\section{Experiments and Benchmarks}
In this section, we present the different experiments we made. The code is accessible online and will be disclosed after peer review.

We first present a comparison of the resistance of various DDFs against saturation, without any sliding window. This comparison helps us to choose the best subfilter.

Then, we design an experiment in which we compare the error rates of a QHT and a queuing filter, using the same QHT as a subfilter. 

\subsection{Saturation Resistance of DDFs}\label{sub:saturation}
In this section, we compare the resistance of various filters to saturation. We take the same filters and parameters as \cite{10.1145/3297280.3297335}, namely:

\begin{itemize}
	\item QHT \cite{10.1145/3297280.3297335}, 1 bucket per row, 3 bits per fingerprint.
	\item SQF \cite{Dut13}, 1 bucket per row, $r = 2$ and $r' = 1$
	\item Cuckoo Filter \cite{Fan14}, cells containing 1 element of 3 bits each
	\item Stable Bloom Filter (SBF) \cite{Den06}, 2 bits per cell, 2 hash functions, targeted FPR of 0.02
	\item $A2$ Filter \cite{Yoo10}, targeted FPR of $0.1$ on the sliding window.
	\item Block-Decaying Bloom Filter (b\_DBF) \cite{She08}, sliding window of 6000 elements.
\end{itemize}

The stream was a stream of uniform random elements, taken from an alphabet of $2^{26}$ elements, and amounts for about 8\% duplicates on the 150 000 000 elements in the longest stream used. Results are plotted in \ref{fig:graph_n}.

\begin{figure}
	\input{er_n_artificial}
	\caption{Error rate (times 100) of DDFs of 1Mb, depending on the size of the stream, without any sliding window. Hatched area represents over-optimal (impossible) values}.\label{fig:graph_n}
\end{figure}

\subsection{Benchmarking Various Queuing Filters}
In this section, we experimentally prove the heuristic used in \ref{sub:optimal_ddf}, that the best DDF operating on a full stream leads to the best queueing filter operating on a sliding window.

\todo

\subsection{Benchmark of the influence of $L$ parameter}\label{sub:L_bench}
In this section, we make experiments on the influence of $L$ on the error rate of the queueing filter. 
\subsection{Comparison of QHT and QHT-Queueing Filter}
In this section, we compare the efficiency of QHT and QHT-queueing filter. For this, we use both an artificial dataset, as well as a real one, on various sliding window sizes.




\section{Security}
%% Acknowledgements 
%\begin{acks} \end{acks}

%% Bibliography
\bibliographystyle{ACM-Reference-Format}
\bibliography{qhtv2}

%% Appendix
%\appendix

\end{document}
